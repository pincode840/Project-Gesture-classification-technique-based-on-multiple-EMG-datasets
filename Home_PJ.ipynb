{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688ab5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9059e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ninapro ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "ninapro_df = pd.DataFrame()\n",
    "for i in range (1,11):\n",
    "    adress = f\"ninapro_db5/s{i}/S{i}_E2_A1\"\n",
    "    filename = adress\n",
    "    mat = sio.loadmat(filename)\n",
    "    emg = mat['emg']\n",
    "    Restimulus = mat['restimulus']\n",
    "    rerepetition = mat['rerepetition']\n",
    "    df_emg = pd.DataFrame(emg)\n",
    "    df_Restimulus = pd.DataFrame(Restimulus)\n",
    "#    df_rerepetition = pd.DataFrame(rerepetition)\n",
    "    df = pd.concat([df_emg, df_Restimulus], axis=1)\n",
    "#    df = pd.concat([df, df_rerepetition], axis=1)\n",
    "#    df.columns = ['emg1', 'emg2', 'emg3', 'emg4', 'emg5', 'emg6', 'emg7', 'emg8', 'emg9', 'emg10', 'emg11', 'emg12', 'emg13', 'emg14', 'emg15', 'emg16', 'Restimulus', 'rerepetition']\n",
    "    df.columns = ['emg1', 'emg2', 'emg3', 'emg4', 'emg5', 'emg6', 'emg7', 'emg8', 'emg9', 'emg10', 'emg11', 'emg12', 'emg13', 'emg14', 'emg15', 'emg16', 'Restimulus']\n",
    "    ninapro_df = pd.concat([ninapro_df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d8918c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„¤ì • í™•ì¸: 200Hz -> 1000Hz ë³€í™˜ (ë¹„ìœ¨: 5.0ë°°)\n",
      "ë°ì´í„° ë³€í™˜ ì§„í–‰ ì¤‘ (Splitting -> Upsampling -> Normalization)...\n",
      "------------------------------\n",
      "ë³€í™˜ ì™„ë£Œ!\n",
      "ì´ ìƒ˜í”Œ(ë™ì‘) ê°œìˆ˜: 1020\n",
      "ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (Max Time Steps): 7245\n",
      "ì…ë ¥ ë°ì´í„° Shape (X): (1020, 7245, 16)\n",
      "ë¼ë²¨ ë°ì´í„° Shape (y): (1020,)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#upsampling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_emg_data(df):\n",
    "   \n",
    "    # ì„¤ì • ê°’\n",
    "    ORIGINAL_FS = 200   # ê¸°ì¡´ ì£¼íŒŒìˆ˜\n",
    "    TARGET_FS = 1000    # ëª©í‘œ ì£¼íŒŒìˆ˜\n",
    "    RESAMPLE_RATIO = TARGET_FS / ORIGINAL_FS # 5.0\n",
    "\n",
    "    # 1. ë°ì´í„°ì™€ ë¼ë²¨ ë¶„ë¦¬\n",
    "    # ë§ˆì§€ë§‰ ì»¬ëŸ¼ì´ Restimulusë¼ê³  ê°€ì •\n",
    "    feature_cols = df.columns[:-1]  # EMG ì±„ë„ 16ê°œ\n",
    "    label_col = df.columns[-1]      # Restimulus\n",
    "    \n",
    "    print(f\"ì„¤ì • í™•ì¸: {ORIGINAL_FS}Hz -> {TARGET_FS}Hz ë³€í™˜ (ë¹„ìœ¨: {RESAMPLE_RATIO}ë°°)\")\n",
    "    \n",
    "    # 2. ë™ì‘(Class)ë³„ êµ¬ê°„ ìë¥´ê¸°\n",
    "    # ë‹¨ìˆœíˆ ë¼ë²¨ê°’(0,1,2..)ìœ¼ë¡œ ë¬¶ëŠ”ê²Œ ì•„ë‹ˆë¼, ì‹œê°„ ìˆœì„œìƒ 'ì—°ì†ëœ ë™ì‘' ë‹¨ìœ„ë¡œ ì˜ë¼ì•¼ í•¨\n",
    "    # ë¼ë²¨ì´ ë³€í•˜ëŠ” ì§€ì ë§ˆë‹¤ ìƒˆë¡œìš´ ê·¸ë£¹ ID ë¶€ì—¬\n",
    "    df['segment_id'] = (df[label_col] != df[label_col].shift()).cumsum()\n",
    "    \n",
    "    processed_data_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    # ê·¸ë£¹ë³„ ì²˜ë¦¬\n",
    "    print(\"ë°ì´í„° ë³€í™˜ ì§„í–‰ ì¤‘ (Splitting -> Upsampling -> Normalization)...\")\n",
    "    \n",
    "    for _, group in df.groupby('segment_id'):\n",
    "        # í˜„ì¬ êµ¬ê°„ì˜ ë¼ë²¨\n",
    "        current_label = group[label_col].iloc[0]\n",
    "        \n",
    "        # (ì„ íƒì‚¬í•­) 0ë²ˆ í´ë˜ìŠ¤(Rest/íœ´ì‹)ê°€ í•™ìŠµì— ë¶ˆí•„ìš”í•˜ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œí•˜ì—¬ ê±´ë„ˆë›°ê¸°\n",
    "        if current_label == 0: continue\n",
    "        \n",
    "        # í•´ë‹¹ êµ¬ê°„ì˜ EMG ë°ì´í„° ì¶”ì¶œ (Numpy ë³€í™˜)\n",
    "        raw_signal = group[feature_cols].values\n",
    "        \n",
    "        # 3. 1000Hz Upsampling\n",
    "        # ë³€í™˜ í›„ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°: í˜„ì¬ ê¸¸ì´ * 5\n",
    "        new_length = int(len(raw_signal) * RESAMPLE_RATIO)\n",
    "        \n",
    "        if new_length > 0:\n",
    "            # Fourier methodë¥¼ ì´ìš©í•œ ë¦¬ìƒ˜í”Œë§ (ì‹ í˜¸ ì†ì‹¤ ìµœì†Œí™”)\n",
    "            upsampled_signal = signal.resample(raw_signal, new_length)\n",
    "            \n",
    "            # 4. Z-score ì •ê·œí™” (Standardization)\n",
    "            # ê° ë™ì‘ êµ¬ê°„(Segment)ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ ì •ê·œí™” ìˆ˜í–‰\n",
    "            scaler = StandardScaler()\n",
    "            normalized_signal = scaler.fit_transform(upsampled_signal)\n",
    "            \n",
    "            processed_data_list.append(normalized_signal)\n",
    "            labels_list.append(current_label)\n",
    "            \n",
    "    # 5. ëª¨ë¸ ì…ë ¥ì„ ìœ„í•œ Padding ë° Numpy Stacking\n",
    "    # ëª¨ë¸ì— ë„£ìœ¼ë ¤ë©´ ëª¨ë“  ë°ì´í„°ì˜ Time Step(ê¸¸ì´)ì´ ê°™ì•„ì•¼ í•¨.\n",
    "    # ê°€ì¥ ê¸´ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ ì±„ì›€ (Zero-padding)\n",
    "    \n",
    "    max_seq_len = max(len(d) for d in processed_data_list)\n",
    "    num_samples = len(processed_data_list)\n",
    "    num_channels = len(feature_cols) # 16\n",
    "    \n",
    "    # ê²°ê³¼ ë‹´ì„ ë¹ˆ ë°°ì—´ ìƒì„± (Batch, Time, Channel)\n",
    "    X_padded = np.zeros((num_samples, max_seq_len, num_channels))\n",
    "    \n",
    "    # ë°ì´í„° ì±„ì›Œë„£ê¸°\n",
    "    for i, data in enumerate(processed_data_list):\n",
    "        length = len(data)\n",
    "        X_padded[i, :length, :] = data  # ì•ìª½ë¶€í„° ë°ì´í„° ì±„ì›€ (Post-padding 0)\n",
    "        \n",
    "    y_labels = np.array(labels_list)\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(\"ë³€í™˜ ì™„ë£Œ!\")\n",
    "    print(f\"ì´ ìƒ˜í”Œ(ë™ì‘) ê°œìˆ˜: {num_samples}\")\n",
    "    print(f\"ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (Max Time Steps): {max_seq_len}\")\n",
    "    print(f\"ì…ë ¥ ë°ì´í„° Shape (X): {X_padded.shape}\") # (Batch, Time, 16)\n",
    "    print(f\"ë¼ë²¨ ë°ì´í„° Shape (y): {y_labels.shape}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    return X_padded, y_labels\n",
    "\n",
    "\n",
    "X_up, y_up = preprocess_emg_data(ninapro_df)\n",
    "\n",
    "# 3. ëª¨ë¸ ì…ë ¥ í™•ì¸\n",
    "# 1D CNN + Attention ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ 'X'ë¥¼, íƒ€ê²Ÿìœ¼ë¡œ 'y'ë¥¼ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86cd14ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nature_df1110 ì¢…ë£Œ\n",
      "nature_df1111 ì¢…ë£Œ\n",
      "nature_df1112 ì¢…ë£Œ\n",
      "nature_df1113 ì¢…ë£Œ\n",
      "nature_df1114 ì¢…ë£Œ\n",
      "nature_df1115 ì¢…ë£Œ\n",
      "nature_df1116 ì¢…ë£Œ\n",
      "nature_df1117 ì¢…ë£Œ\n",
      "nature_df1118 ì¢…ë£Œ\n",
      "nature_df1119 ì¢…ë£Œ\n",
      "nature_df11110 ì¢…ë£Œ\n",
      "nature_df11111 ì¢…ë£Œ\n",
      "nature_df11112 ì¢…ë£Œ\n",
      "nature_df11113 ì¢…ë£Œ\n",
      "nature_df11114 ì¢…ë£Œ\n",
      "nature_df11115 ì¢…ë£Œ\n",
      "nature_df11116 ì¢…ë£Œ\n",
      "nature_df11117 ì¢…ë£Œ\n",
      "nature_df11118 ì¢…ë£Œ\n",
      "nature_df11119 ì¢…ë£Œ\n",
      "nature_df11120 ì¢…ë£Œ\n",
      "nature_df11121 ì¢…ë£Œ\n",
      "nature_df11122 ì¢…ë£Œ\n",
      "nature_df11123 ì¢…ë£Œ\n",
      "nature_df11124 ì¢…ë£Œ\n",
      "nature_df11125 ì¢…ë£Œ\n",
      "nature_df11126 ì¢…ë£Œ\n",
      "nature_df11127 ì¢…ë£Œ\n",
      "nature_df11128 ì¢…ë£Œ\n",
      "nature_df11129 ì¢…ë£Œ\n",
      "nature_df11130 ì¢…ë£Œ\n",
      "nature_df11131 ì¢…ë£Œ\n",
      "nature_df11132 ì¢…ë£Œ\n",
      "nature_df11133 ì¢…ë£Œ\n",
      "nature_df11134 ì¢…ë£Œ\n",
      "nature_df11135 ì¢…ë£Œ\n",
      "nature_df11136 ì¢…ë£Œ\n",
      "nature_df11137 ì¢…ë£Œ\n",
      "nature_df11138 ì¢…ë£Œ\n",
      "nature_df11139 ì¢…ë£Œ\n",
      "nature_df11140 ì¢…ë£Œ\n",
      "nature_df11141 ì¢…ë£Œ\n",
      "nature_df11142 ì¢…ë£Œ\n",
      "nature_df11143 ì¢…ë£Œ\n",
      "nature_df11144 ì¢…ë£Œ\n",
      "nature_df11145 ì¢…ë£Œ\n",
      "nature_df11146 ì¢…ë£Œ\n",
      "nature_df11147 ì¢…ë£Œ\n",
      "nature_df11148 ì¢…ë£Œ\n",
      "nature_df11149 ì¢…ë£Œ\n",
      "nature_df11150 ì¢…ë£Œ\n",
      "nature_df11151 ì¢…ë£Œ\n",
      "nature_df11152 ì¢…ë£Œ\n",
      "nature_df11153 ì¢…ë£Œ\n",
      "nature_df11154 ì¢…ë£Œ\n",
      "nature_df11155 ì¢…ë£Œ\n",
      "nature_df11156 ì¢…ë£Œ\n",
      "nature_df11157 ì¢…ë£Œ\n",
      "nature_df11158 ì¢…ë£Œ\n",
      "nature_df11159 ì¢…ë£Œ\n",
      "nature_df11160 ì¢…ë£Œ\n",
      "nature_df11161 ì¢…ë£Œ\n",
      "nature_df11162 ì¢…ë£Œ\n",
      "nature_df11163 ì¢…ë£Œ\n",
      "nature_df11164 ì¢…ë£Œ\n",
      "nature_df11165 ì¢…ë£Œ\n",
      "nature_df11166 ì¢…ë£Œ\n",
      "nature_df11167 ì¢…ë£Œ\n",
      "nature_df11168 ì¢…ë£Œ\n",
      "nature_df11169 ì¢…ë£Œ\n",
      "nature_df11170 ì¢…ë£Œ\n",
      "nature_df11171 ì¢…ë£Œ\n",
      "nature_df11172 ì¢…ë£Œ\n",
      "nature_df11173 ì¢…ë£Œ\n",
      "nature_df11174 ì¢…ë£Œ\n",
      "nature_df11175 ì¢…ë£Œ\n",
      "nature_df11176 ì¢…ë£Œ\n",
      "nature_df11177 ì¢…ë£Œ\n",
      "nature_df11178 ì¢…ë£Œ\n",
      "nature_df11179 ì¢…ë£Œ\n",
      "nature_df11180 ì¢…ë£Œ\n",
      "nature_df11181 ì¢…ë£Œ\n",
      "nature_df11182 ì¢…ë£Œ\n",
      "nature_df11183 ì¢…ë£Œ\n",
      "nature_df11184 ì¢…ë£Œ\n",
      "nature_df11185 ì¢…ë£Œ\n",
      "nature_df11186 ì¢…ë£Œ\n",
      "nature_df11187 ì¢…ë£Œ\n",
      "nature_df11188 ì¢…ë£Œ\n",
      "nature_df11189 ì¢…ë£Œ\n",
      "nature_df11190 ì¢…ë£Œ\n",
      "nature_df11191 ì¢…ë£Œ\n",
      "nature_df11192 ì¢…ë£Œ\n",
      "nature_df11193 ì¢…ë£Œ\n",
      "nature_df11194 ì¢…ë£Œ\n",
      "nature_df11195 ì¢…ë£Œ\n",
      "nature_df11196 ì¢…ë£Œ\n",
      "nature_df11197 ì¢…ë£Œ\n",
      "nature_df11198 ì¢…ë£Œ\n",
      "nature_df11199 ì¢…ë£Œ\n",
      "nature_df111100 ì¢…ë£Œ\n",
      "nature_df111101 ì¢…ë£Œ\n",
      "nature_df111102 ì¢…ë£Œ\n",
      "nature_df111103 ì¢…ë£Œ\n",
      "nature_df111104 ì¢…ë£Œ\n",
      "nature_df111105 ì¢…ë£Œ\n",
      "nature_df111106 ì¢…ë£Œ\n",
      "nature_df111107 ì¢…ë£Œ\n",
      "nature_df111108 ì¢…ë£Œ\n",
      "nature_df111109 ì¢…ë£Œ\n",
      "nature_df111110 ì¢…ë£Œ\n",
      "nature_df111111 ì¢…ë£Œ\n",
      "nature_df111112 ì¢…ë£Œ\n",
      "nature_df111113 ì¢…ë£Œ\n",
      "nature_df111114 ì¢…ë£Œ\n",
      "nature_df111115 ì¢…ë£Œ\n",
      "nature_df111116 ì¢…ë£Œ\n",
      "nature_df111117 ì¢…ë£Œ\n",
      "nature_df111118 ì¢…ë£Œ\n",
      "nature_df111119 ì¢…ë£Œ\n",
      "nature_df111120 ì¢…ë£Œ\n",
      "nature_df111121 ì¢…ë£Œ\n",
      "nature_df111122 ì¢…ë£Œ\n",
      "nature_df111123 ì¢…ë£Œ\n",
      "nature_df111124 ì¢…ë£Œ\n",
      "nature_df111125 ì¢…ë£Œ\n",
      "nature_df111126 ì¢…ë£Œ\n",
      "nature_df111127 ì¢…ë£Œ\n",
      "nature_df111128 ì¢…ë£Œ\n",
      "nature_df111129 ì¢…ë£Œ\n",
      "nature_df111130 ì¢…ë£Œ\n",
      "nature_df111131 ì¢…ë£Œ\n",
      "nature_df111132 ì¢…ë£Œ\n",
      "nature_df111133 ì¢…ë£Œ\n",
      "nature_df111134 ì¢…ë£Œ\n",
      "nature_df111135 ì¢…ë£Œ\n",
      "nature_df111136 ì¢…ë£Œ\n",
      "nature_df111137 ì¢…ë£Œ\n",
      "nature_df111138 ì¢…ë£Œ\n",
      "nature_df111139 ì¢…ë£Œ\n",
      "nature_df111140 ì¢…ë£Œ\n",
      "nature_df111141 ì¢…ë£Œ\n",
      "nature_df111142 ì¢…ë£Œ\n",
      "nature_df111143 ì¢…ë£Œ\n",
      "nature_df111144 ì¢…ë£Œ\n",
      "nature_df111145 ì¢…ë£Œ\n",
      "nature_df111146 ì¢…ë£Œ\n",
      "nature_df111147 ì¢…ë£Œ\n",
      "nature_df111148 ì¢…ë£Œ\n",
      "nature_df111149 ì¢…ë£Œ\n",
      "nature_df1120 ì¢…ë£Œ\n",
      "nature_df1121 ì¢…ë£Œ\n",
      "nature_df1122 ì¢…ë£Œ\n",
      "nature_df1123 ì¢…ë£Œ\n",
      "nature_df1124 ì¢…ë£Œ\n",
      "nature_df1125 ì¢…ë£Œ\n",
      "nature_df1126 ì¢…ë£Œ\n",
      "nature_df1127 ì¢…ë£Œ\n",
      "nature_df1128 ì¢…ë£Œ\n",
      "nature_df1129 ì¢…ë£Œ\n",
      "nature_df11210 ì¢…ë£Œ\n",
      "nature_df11211 ì¢…ë£Œ\n",
      "nature_df11212 ì¢…ë£Œ\n",
      "nature_df11213 ì¢…ë£Œ\n",
      "nature_df11214 ì¢…ë£Œ\n",
      "nature_df11215 ì¢…ë£Œ\n",
      "nature_df11216 ì¢…ë£Œ\n",
      "nature_df11217 ì¢…ë£Œ\n",
      "nature_df11218 ì¢…ë£Œ\n",
      "nature_df11219 ì¢…ë£Œ\n",
      "nature_df11220 ì¢…ë£Œ\n",
      "nature_df11221 ì¢…ë£Œ\n",
      "nature_df11222 ì¢…ë£Œ\n",
      "nature_df11223 ì¢…ë£Œ\n",
      "nature_df11224 ì¢…ë£Œ\n",
      "nature_df11225 ì¢…ë£Œ\n",
      "nature_df11226 ì¢…ë£Œ\n",
      "nature_df11227 ì¢…ë£Œ\n",
      "nature_df11228 ì¢…ë£Œ\n",
      "nature_df11229 ì¢…ë£Œ\n",
      "nature_df11230 ì¢…ë£Œ\n",
      "nature_df11231 ì¢…ë£Œ\n",
      "nature_df11232 ì¢…ë£Œ\n",
      "nature_df11233 ì¢…ë£Œ\n",
      "nature_df11234 ì¢…ë£Œ\n",
      "nature_df11235 ì¢…ë£Œ\n",
      "nature_df11236 ì¢…ë£Œ\n",
      "nature_df11237 ì¢…ë£Œ\n",
      "nature_df11238 ì¢…ë£Œ\n",
      "nature_df11239 ì¢…ë£Œ\n",
      "nature_df11240 ì¢…ë£Œ\n",
      "nature_df11241 ì¢…ë£Œ\n",
      "nature_df11242 ì¢…ë£Œ\n",
      "nature_df11243 ì¢…ë£Œ\n",
      "nature_df11244 ì¢…ë£Œ\n",
      "nature_df11245 ì¢…ë£Œ\n",
      "nature_df11246 ì¢…ë£Œ\n",
      "nature_df11247 ì¢…ë£Œ\n",
      "nature_df11248 ì¢…ë£Œ\n",
      "nature_df11249 ì¢…ë£Œ\n",
      "nature_df11250 ì¢…ë£Œ\n",
      "nature_df11251 ì¢…ë£Œ\n",
      "nature_df11252 ì¢…ë£Œ\n",
      "nature_df11253 ì¢…ë£Œ\n",
      "nature_df11254 ì¢…ë£Œ\n",
      "nature_df11255 ì¢…ë£Œ\n",
      "nature_df11256 ì¢…ë£Œ\n",
      "nature_df11257 ì¢…ë£Œ\n",
      "nature_df11258 ì¢…ë£Œ\n",
      "nature_df11259 ì¢…ë£Œ\n",
      "nature_df11260 ì¢…ë£Œ\n",
      "nature_df11261 ì¢…ë£Œ\n",
      "nature_df11262 ì¢…ë£Œ\n",
      "nature_df11263 ì¢…ë£Œ\n",
      "nature_df11264 ì¢…ë£Œ\n",
      "nature_df11265 ì¢…ë£Œ\n",
      "nature_df11266 ì¢…ë£Œ\n",
      "nature_df11267 ì¢…ë£Œ\n",
      "nature_df11268 ì¢…ë£Œ\n",
      "nature_df11269 ì¢…ë£Œ\n",
      "nature_df11270 ì¢…ë£Œ\n",
      "nature_df11271 ì¢…ë£Œ\n",
      "nature_df11272 ì¢…ë£Œ\n",
      "nature_df11273 ì¢…ë£Œ\n",
      "nature_df11274 ì¢…ë£Œ\n",
      "nature_df11275 ì¢…ë£Œ\n",
      "nature_df11276 ì¢…ë£Œ\n",
      "nature_df11277 ì¢…ë£Œ\n",
      "nature_df11278 ì¢…ë£Œ\n",
      "nature_df11279 ì¢…ë£Œ\n",
      "nature_df11280 ì¢…ë£Œ\n",
      "nature_df11281 ì¢…ë£Œ\n",
      "nature_df11282 ì¢…ë£Œ\n",
      "nature_df11283 ì¢…ë£Œ\n",
      "nature_df11284 ì¢…ë£Œ\n",
      "nature_df11285 ì¢…ë£Œ\n",
      "nature_df11286 ì¢…ë£Œ\n",
      "nature_df11287 ì¢…ë£Œ\n",
      "nature_df11288 ì¢…ë£Œ\n",
      "nature_df11289 ì¢…ë£Œ\n",
      "nature_df11290 ì¢…ë£Œ\n",
      "nature_df11291 ì¢…ë£Œ\n",
      "nature_df11292 ì¢…ë£Œ\n",
      "nature_df11293 ì¢…ë£Œ\n",
      "nature_df11294 ì¢…ë£Œ\n",
      "nature_df11295 ì¢…ë£Œ\n",
      "nature_df11296 ì¢…ë£Œ\n",
      "nature_df11297 ì¢…ë£Œ\n",
      "nature_df11298 ì¢…ë£Œ\n",
      "nature_df11299 ì¢…ë£Œ\n",
      "nature_df112100 ì¢…ë£Œ\n",
      "nature_df112101 ì¢…ë£Œ\n",
      "nature_df112102 ì¢…ë£Œ\n",
      "nature_df112103 ì¢…ë£Œ\n",
      "nature_df112104 ì¢…ë£Œ\n",
      "nature_df112105 ì¢…ë£Œ\n",
      "nature_df112106 ì¢…ë£Œ\n",
      "nature_df112107 ì¢…ë£Œ\n",
      "nature_df112108 ì¢…ë£Œ\n",
      "nature_df112109 ì¢…ë£Œ\n",
      "nature_df112110 ì¢…ë£Œ\n",
      "nature_df112111 ì¢…ë£Œ\n",
      "nature_df112112 ì¢…ë£Œ\n",
      "nature_df112113 ì¢…ë£Œ\n",
      "nature_df112114 ì¢…ë£Œ\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m data_parame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrasp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data_parame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrasp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(grasp_mapping)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m150\u001b[39m):\n\u001b[1;32m---> 20\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnature_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43ml\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     21\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[0;32m     22\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRestimulus\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\dlrmflf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\h5py\\_hl\\dataset.py:1144\u001b[0m, in \u001b[0;36mDataset.__array__\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   1140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1141\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset.__array__ received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcopy\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut memory allocation cannot be avoided on read\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1143\u001b[0m     )\n\u001b[1;32m-> 1144\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;66;03m# Special case for (0,)*-shape datasets\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#nature ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "grasp_mapping = {\n",
    "    1: 6,\n",
    "    2: 18,\n",
    "    3: 7,\n",
    "    4: 5,\n",
    "    5: 19,\n",
    "    6: 0\n",
    "}\n",
    "\n",
    "nature_df = pd.DataFrame()\n",
    "for i in range(1, 9):\n",
    "    for j in range(1,3):\n",
    "        for k in range(1,3):\n",
    "            filename = fr'nature_data\\data\\participant_{i}\\participant{i}_day{j}_block{k}\\emg_data.hdf5'\n",
    "            data_parame = pd.read_csv(fr'nature_data\\data\\participant_{i}\\participant{i}_day{j}_block{k}\\trials.csv') \n",
    "            nature_data = h5py.File(filename, 'r')\n",
    "            data_parame['grasp'] = data_parame['grasp'].replace(grasp_mapping)\n",
    "            for l in range(0, 150):\n",
    "                df = pd.DataFrame(np.array(nature_data[f\"{l}\"]))\n",
    "                df=df.transpose()\n",
    "                df['Restimulus'] = ''\n",
    "                df['Restimulus'] = data_parame['grasp'].iloc[l]\n",
    "                nature_df = pd.concat([nature_df, df], axis=0)\n",
    "                print(f\"nature_df{i}{j}{k}{l} ì¢…ë£Œ\")\n",
    "nature_df.columns = ['emg1', 'emg2', 'emg3', 'emg4', 'emg5', 'emg6', 'emg7', 'emg8', 'emg9', 'emg10', 'emg11', 'emg12', 'emg13', 'emg14', 'emg15', 'emg16', 'Restimulus']\n",
    "#nature_df.to_csv('nature_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e06b70f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emg1</th>\n",
       "      <th>emg2</th>\n",
       "      <th>emg3</th>\n",
       "      <th>emg4</th>\n",
       "      <th>emg5</th>\n",
       "      <th>emg6</th>\n",
       "      <th>emg7</th>\n",
       "      <th>emg8</th>\n",
       "      <th>emg9</th>\n",
       "      <th>emg10</th>\n",
       "      <th>emg11</th>\n",
       "      <th>emg12</th>\n",
       "      <th>emg13</th>\n",
       "      <th>emg14</th>\n",
       "      <th>emg15</th>\n",
       "      <th>emg16</th>\n",
       "      <th>Restimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47973915</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47973916</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47973917</th>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47973918</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47973919</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47973920 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              emg1      emg2      emg3      emg4      emg5      emg6  \\\n",
       "0         0.000038  0.000025  0.000008  0.000008 -0.000016 -0.000002   \n",
       "1         0.000020  0.000026  0.000010  0.000008 -0.000017 -0.000002   \n",
       "2         0.000009  0.000026  0.000011  0.000008 -0.000024 -0.000004   \n",
       "3         0.000012  0.000023  0.000009  0.000007 -0.000041 -0.000009   \n",
       "4         0.000021  0.000020  0.000006  0.000005 -0.000044 -0.000015   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "47973915  0.000030  0.000042  0.000010  0.000012 -0.000030 -0.000011   \n",
       "47973916  0.000030  0.000060  0.000011  0.000013 -0.000030 -0.000009   \n",
       "47973917  0.000024  0.000074  0.000012  0.000014 -0.000029 -0.000006   \n",
       "47973918  0.000015  0.000077  0.000013  0.000016 -0.000027 -0.000005   \n",
       "47973919  0.000002  0.000076  0.000014  0.000018 -0.000026 -0.000006   \n",
       "\n",
       "              emg7      emg8      emg9     emg10     emg11     emg12  \\\n",
       "0        -0.000005 -0.000010  0.000017  0.000061 -0.000008 -0.000008   \n",
       "1        -0.000008 -0.000007  0.000014  0.000043 -0.000011 -0.000009   \n",
       "2        -0.000013 -0.000005  0.000014  0.000025 -0.000015 -0.000011   \n",
       "3        -0.000016 -0.000006  0.000012  0.000010 -0.000017 -0.000012   \n",
       "4        -0.000018 -0.000008  0.000007  0.000003 -0.000016 -0.000011   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "47973915 -0.000027 -0.000002 -0.000092 -0.000030 -0.000013 -0.000016   \n",
       "47973916 -0.000026 -0.000013 -0.000086 -0.000012 -0.000010 -0.000015   \n",
       "47973917 -0.000025 -0.000014 -0.000072  0.000002 -0.000010 -0.000016   \n",
       "47973918 -0.000021 -0.000014 -0.000051  0.000002 -0.000012 -0.000017   \n",
       "47973919 -0.000016 -0.000017 -0.000028 -0.000007 -0.000014 -0.000018   \n",
       "\n",
       "             emg13     emg14     emg15     emg16  Restimulus  \n",
       "0        -0.000022 -0.000008 -0.000016 -0.000013           7  \n",
       "1        -0.000022 -0.000012 -0.000016 -0.000018           7  \n",
       "2        -0.000018 -0.000013 -0.000013 -0.000016           7  \n",
       "3        -0.000014 -0.000013 -0.000008 -0.000013           7  \n",
       "4        -0.000012 -0.000014 -0.000006 -0.000012           7  \n",
       "...            ...       ...       ...       ...         ...  \n",
       "47973915 -0.000010 -0.000008 -0.000013 -0.000011          18  \n",
       "47973916 -0.000009 -0.000008 -0.000014 -0.000011          18  \n",
       "47973917 -0.000011 -0.000008 -0.000014 -0.000013          18  \n",
       "47973918 -0.000013 -0.000006 -0.000013 -0.000014          18  \n",
       "47973919 -0.000014 -0.000006 -0.000012 -0.000015          18  \n",
       "\n",
       "[47973920 rows x 17 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nature df csvë¶ˆëŸ¬ì˜¤ëŠ” ì½”ë“œ\n",
    "nature_df = pd.read_csv('nature_df.csv')\n",
    "nature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d04ecbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ì²˜ë¦¬ ì‹œì‘: 47973920 rows\n",
      "Sampling Rate ë³€í™˜: 2000Hz -> 1000Hz (Downsampling)\n",
      "ì´ 815ê°œì˜ ë™ì‘ ì„¸ê·¸ë¨¼íŠ¸ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ë³€í™˜ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Segments: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 815/815 [00:29<00:00, 27.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "íŒ¨ë”©(Padding) ë° ìµœì¢… ë°°ì—´ ìƒì„± ì¤‘...\n",
      "----------------------------------------\n",
      "ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ì…ë ¥ ë°ì´í„° Shape (X): (783, 50040, 16)\n",
      "ë¼ë²¨ ë°ì´í„° Shape (y): (783,)\n",
      "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰(ì˜ˆìƒ): 2.34 GB\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#down sampling code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm  # ì§„í–‰ ìƒí™© í‘œì‹œìš© (ì„¤ì¹˜ í•„ìš”: pip install tqdm)\n",
    "\n",
    "def process_large_emg_data(df):\n",
    "    \"\"\"\n",
    "    4800ë§Œ í–‰ ê·œëª¨ì˜ 2000Hz EMG ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ 1D CNN ëª¨ë¸ ì…ë ¥ìš© Numpy ë°°ì—´ë¡œ ë³€í™˜\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): 2000Hz Raw ë°ì´í„° (Rows x 17 cols)\n",
    "                           (1~16 col: EMG ì„¼ì„œ, 17 col: Restimulus í´ë˜ìŠ¤)\n",
    "    \n",
    "    Returns:\n",
    "        X_padded (np.array): (Batch_Size, Max_Time_Steps, 16)\n",
    "        y_labels (np.array): (Batch_Size,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 1. ì„¤ì • ë° ì¤€ë¹„\n",
    "    # ---------------------------------------------------------\n",
    "    ORIGINAL_FS = 2000  # ì›ë³¸ ì£¼íŒŒìˆ˜\n",
    "    TARGET_FS = 1000    # ëª©í‘œ ì£¼íŒŒìˆ˜\n",
    "    RATIO = TARGET_FS / ORIGINAL_FS  # 0.5 (ì ˆë°˜ìœ¼ë¡œ ì¶•ì†Œ)\n",
    "\n",
    "    feature_cols = df.columns[:-1]  # ì•ì˜ 16ê°œ ì»¬ëŸ¼ (EMG)\n",
    "    label_col = df.columns[-1]      # ë§ˆì§€ë§‰ ì»¬ëŸ¼ (Restimulus)\n",
    "    \n",
    "    print(f\"ë°ì´í„° ì²˜ë¦¬ ì‹œì‘: {len(df)} rows\")\n",
    "    print(f\"Sampling Rate ë³€í™˜: {ORIGINAL_FS}Hz -> {TARGET_FS}Hz (Downsampling)\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  (Segmentation)\n",
    "    # Restimulus ê°’ì´ ë°”ë€ŒëŠ” ì§€ì ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹ ID ìƒì„±\n",
    "    # ---------------------------------------------------------\n",
    "    # ëŒ€ìš©ëŸ‰ ë°ì´í„°ì´ë¯€ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì‚¬ìš©í•˜ì—¬ ì—°ì‚°\n",
    "    df['segment_id'] = (df[label_col] != df[label_col].shift()).cumsum()\n",
    "    \n",
    "    processed_segments = []\n",
    "    labels = []\n",
    "    \n",
    "    # ê·¸ë£¹í•‘ (ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ groupby ê°ì²´ë¥¼ ë°”ë¡œ ìˆœíšŒ)\n",
    "    grouped = df.groupby('segment_id')\n",
    "    \n",
    "    print(f\"ì´ {len(grouped)}ê°œì˜ ë™ì‘ ì„¸ê·¸ë¨¼íŠ¸ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ë³€í™˜ ì¤‘...\")\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 3. ë°˜ë³µ ì²˜ë¦¬ (Downsampling -> Normalization)\n",
    "    # ---------------------------------------------------------\n",
    "    for _, group in tqdm(grouped, desc=\"Processing Segments\"):\n",
    "        # í˜„ì¬ í´ë˜ìŠ¤ ë¼ë²¨\n",
    "        current_label = group[label_col].iloc[0]\n",
    "        \n",
    "        # (ì˜µì…˜) 0ë²ˆ(íœ´ì‹) í´ë˜ìŠ¤ë¥¼ ì œì™¸í•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ\n",
    "        if current_label == 0: continue\n",
    "\n",
    "        # ì„¼ì„œ ë°ì´í„° ì¶”ì¶œ (Numpy ë³€í™˜)\n",
    "        raw_signals = group[feature_cols].to_numpy() # .values ë³´ë‹¤ to_numpy() ê¶Œì¥\n",
    "        \n",
    "        # ë°ì´í„° ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´(ì˜ˆ: ë…¸ì´ì¦ˆ) ìŠ¤í‚µí•˜ëŠ” ë¡œì§ ì¶”ê°€ ê°€ëŠ¥\n",
    "        if len(raw_signals) < 2:\n",
    "            continue\n",
    "\n",
    "        # [Downsampling] 2000Hz -> 1000Hz\n",
    "        # ëª©í‘œ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°\n",
    "        new_num_samples = int(len(raw_signals) * RATIO)\n",
    "        \n",
    "        if new_num_samples > 0:\n",
    "            # Fourier method ë¦¬ìƒ˜í”Œë§ (ë°ì´í„°ê°€ ë§ìœ¼ë¯€ë¡œ signal.decimate(x, 2)ë„ ê³ ë ¤ ê°€ëŠ¥)\n",
    "            # ì—¬ê¸°ì„œëŠ” ê¸¸ì´ë¥¼ ì •í™•íˆ ì œì–´í•˜ê¸° ìœ„í•´ resample ì‚¬ìš©\n",
    "            resampled_signals = signal.resample(raw_signals, new_num_samples)\n",
    "            \n",
    "            # [Z-score Normalization]\n",
    "            # ê° ì„¸ê·¸ë¨¼íŠ¸ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ ì •ê·œí™”\n",
    "            scaler = StandardScaler()\n",
    "            normalized_signals = scaler.fit_transform(resampled_signals)\n",
    "            \n",
    "            # ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ (float32ë¡œ ë³€í™˜í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½ ê¶Œì¥)\n",
    "            processed_segments.append(normalized_signals.astype(np.float32))\n",
    "            labels.append(current_label)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4. íŒ¨ë”© (Padding) ë° ìµœì¢… ë³€í™˜\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"íŒ¨ë”©(Padding) ë° ìµœì¢… ë°°ì—´ ìƒì„± ì¤‘...\")\n",
    "    \n",
    "    # ê°€ì¥ ê¸´ ì‹œí€€ìŠ¤ ê¸¸ì´ ì°¾ê¸°\n",
    "    max_len = max(len(seg) for seg in processed_segments)\n",
    "    num_samples = len(processed_segments)\n",
    "    num_channels = len(feature_cols)\n",
    "    \n",
    "    # ê²°ê³¼ ë°°ì—´ ìƒì„± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ float32 ì‚¬ìš©)\n",
    "    # Shape: (Batch_Size, Max_Time_Steps, Channels)\n",
    "    X_final = np.zeros((num_samples, max_len, num_channels), dtype=np.float32)\n",
    "    y_final = np.array(labels, dtype=np.int32)\n",
    "    \n",
    "    # íŒ¨ë”© ì ìš© (Post-padding: ë’¤ìª½ì„ 0ìœ¼ë¡œ ì±„ì›€)\n",
    "    for i, seg in enumerate(processed_segments):\n",
    "        length = len(seg)\n",
    "        X_final[i, :length, :] = seg\n",
    "        \n",
    "    print(\"-\" * 40)\n",
    "    print(\"ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"ì…ë ¥ ë°ì´í„° Shape (X): {X_final.shape}\")\n",
    "    print(f\"ë¼ë²¨ ë°ì´í„° Shape (y): {y_final.shape}\")\n",
    "    print(f\"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰(ì˜ˆìƒ): {X_final.nbytes / 1024**3:.2f} GB\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    return X_final, y_final\n",
    "\n",
    "# 3. í•¨ìˆ˜ ì‹¤í–‰\n",
    "X_down, y_down = process_large_emg_data(nature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86cf83d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 3. ë°ì´í„° ë³‘í•© (Merging) ===\n",
      "ë³‘í•© ì‹œì‘...\n",
      "ë°ì´í„°ì…‹ 1 Shape: (1020, 7245, 16)\n",
      "ë°ì´í„°ì…‹ 2 Shape: (783, 50040, 16)\n",
      "í†µí•© í›„ ëª©í‘œ Shape: (1803, 50040, 16)\n",
      "ë³‘í•© ì™„ë£Œ.\n",
      "----------------------------------------\n",
      "ìµœì¢… ëª¨ë¸ ì…ë ¥ ë°ì´í„° Shape (X): (1803, 50040, 16)\n",
      "ìµœì¢… ëª¨ë¸ ë¼ë²¨ ë°ì´í„° Shape (y): (1803,)\n",
      "ë°ì´í„° íƒ€ì…: float32\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#combin data code\n",
    "import numpy as np\n",
    "import gc  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜(ë©”ëª¨ë¦¬ ê´€ë¦¬ìš©)\n",
    "\n",
    "def merge_datasets(X1, y1, X2, y2):\n",
    "    \"\"\"\n",
    "    ì„œë¡œ ë‹¤ë¥¸ ê¸¸ì´ë¥¼ ê°€ì§„ ë‘ ê°œì˜ 3ì°¨ì› Numpy ë°°ì—´ì„ ë³‘í•©í•©ë‹ˆë‹¤.\n",
    "    ê°€ì¥ ê¸´ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ Zero-paddingì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(f\"ë³‘í•© ì‹œì‘...\")\n",
    "    print(f\"ë°ì´í„°ì…‹ 1 Shape: {X1.shape}\")\n",
    "    print(f\"ë°ì´í„°ì…‹ 2 Shape: {X2.shape}\")\n",
    "\n",
    "    # 1. í†µí•© ì°¨ì› ê³„ì‚°\n",
    "    # ìƒ˜í”Œ ìˆ˜ í•©ê³„\n",
    "    total_samples = X1.shape[0] + X2.shape[0]\n",
    "    # ë‘ ë°ì´í„° ì¤‘ ë” ê¸´ ì‹œí€€ìŠ¤ ê¸¸ì´ ì„ íƒ (ì—¬ê¸°ì„œëŠ” 125080 ì˜ˆìƒ)\n",
    "    max_seq_len = max(X1.shape[1], X2.shape[1])\n",
    "    num_channels = X1.shape[2] # 16\n",
    "\n",
    "    print(f\"í†µí•© í›„ ëª©í‘œ Shape: ({total_samples}, {max_seq_len}, {num_channels})\")\n",
    "    \n",
    "    # 2. ë©”ëª¨ë¦¬ í• ë‹¹ (Float32ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "    # í•œ ë²ˆì— í° ë©”ëª¨ë¦¬ë¥¼ í• ë‹¹í•˜ê³  ë°ì´í„°ë¥¼ ë°€ì–´ë„£ëŠ” ë°©ì‹ì´ ê°€ì¥ íš¨ìœ¨ì ì…ë‹ˆë‹¤.\n",
    "    X_final = np.zeros((total_samples, max_seq_len, num_channels), dtype=np.float32)\n",
    "    y_final = np.zeros((total_samples,), dtype=y1.dtype) # ë¼ë²¨ìš©\n",
    "\n",
    "    # 3. ë°ì´í„° ì±„ì›Œë„£ê¸° (Padding ìë™ ì ìš©)\n",
    "    # X1 (ì—…ìƒ˜í”Œë§ ë°ì´í„°) ë„£ê¸°\n",
    "    # X1ì˜ ê¸¸ì´ë§Œí¼ë§Œ ì•ë¶€ë¶„ì— ì±„ì›Œì§€ê³ , ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ ë‚¨ìŒ (Post-padding íš¨ê³¼)\n",
    "    len1 = X1.shape[1]\n",
    "    X_final[:X1.shape[0], :len1, :] = X1\n",
    "    y_final[:y1.shape[0]] = y1\n",
    "\n",
    "    # X2 (ë‹¤ìš´ìƒ˜í”Œë§ ë°ì´í„°) ë„£ê¸°\n",
    "    # X1ì´ ëë‚œ ì§€ì ë¶€í„° ì‹œì‘\n",
    "    len2 = X2.shape[1]\n",
    "    X_final[X1.shape[0]:, :len2, :] = X2\n",
    "    y_final[y1.shape[0]:] = y2\n",
    "\n",
    "    print(\"ë³‘í•© ì™„ë£Œ.\")\n",
    "    return X_final, y_final\n",
    "\n",
    "# ========================================================\n",
    "# [ì‹¤í–‰ ì½”ë“œ] ë³€ìˆ˜ëª… ë¶„ë¦¬ ë° ë³‘í•©\n",
    "# ========================================================\n",
    "\n",
    "print(\"\\n=== 3. ë°ì´í„° ë³‘í•© (Merging) ===\")\n",
    "# ë³‘í•© í•¨ìˆ˜ ì‹¤í–‰\n",
    "X_combined, y_combined = merge_datasets(X_up, y_up, X_down, y_down)\n",
    "\n",
    "# ë©”ëª¨ë¦¬ í™•ë³´ë¥¼ ìœ„í•´ ì´ì „ ë³€ìˆ˜ ì‚­ì œ (ì„ íƒì‚¬í•­)\n",
    "del X_up, y_up, X_down, y_down\n",
    "gc.collect()\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"ìµœì¢… ëª¨ë¸ ì…ë ¥ ë°ì´í„° Shape (X): {X_combined.shape}\")\n",
    "print(f\"ìµœì¢… ëª¨ë¸ ë¼ë²¨ ë°ì´í„° Shape (y): {y_combined.shape}\")\n",
    "print(f\"ë°ì´í„° íƒ€ì…: {X_combined.dtype}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# ì´ X_combined, y_combinedë¥¼ 1D CNN Encoder Attention ëª¨ë¸ì— ë„£ìœ¼ì‹œë©´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e407eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ì €ì¥ì„ ì‹œì‘í•©ë‹ˆë‹¤. ëŒ€ìƒ í´ë”: ./processed_data\n",
      "1. X_combined.npy ì €ì¥ ì¤‘... (ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)\n",
      "2. y_combined.npy ì €ì¥ ì¤‘...\n",
      "ì¼ë°˜ ì €ì¥ ì™„ë£Œ.\n",
      "3. ì••ì¶• ì €ì¥ ì‹œì‘ (ê¶Œì¥): ./processed_data\\emg_data_compressed.npz ...\n",
      "ëª¨ë“  ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ì €ì¥í•  í´ë” ìƒì„± (í˜„ì¬ ê²½ë¡œì— data í´ë” ìƒì„±)\n",
    "save_dir = './processed_data'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "print(f\"ë°ì´í„° ì €ì¥ì„ ì‹œì‘í•©ë‹ˆë‹¤. ëŒ€ìƒ í´ë”: {save_dir}\")\n",
    "\n",
    "# ========================================================\n",
    "# Method 1: ì¼ë°˜ .npy íŒŒì¼ë¡œ ì €ì¥ (ì†ë„ ë¹ ë¦„, ìš©ëŸ‰ í¼)\n",
    "# ========================================================\n",
    "# X_combined.npyì™€ y_combined.npyë¡œ ê°ê° ì €ì¥ë©ë‹ˆë‹¤.\n",
    "# ì˜ˆìƒ ìš©ëŸ‰: ì•½ 23GB\n",
    "\n",
    "print(\"1. X_combined.npy ì €ì¥ ì¤‘... (ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\n",
    "np.save(os.path.join(save_dir, 'X_combined.npy'), X_combined)\n",
    "\n",
    "print(\"2. y_combined.npy ì €ì¥ ì¤‘...\")\n",
    "np.save(os.path.join(save_dir, 'y_combined.npy'), y_combined)\n",
    "\n",
    "print(\"ì¼ë°˜ ì €ì¥ ì™„ë£Œ.\")\n",
    "\n",
    "# ========================================================\n",
    "# Method 2: ì••ì¶•ëœ .npz íŒŒì¼ë¡œ ì €ì¥ (ê¶Œì¥)\n",
    "# ========================================================\n",
    "# íŒ¨ë”©ìœ¼ë¡œ ì±„ì›Œì§„ 0ì´ ë§ìœ¼ë¯€ë¡œ ìš©ëŸ‰ì´ 1/10 ì´í•˜ë¡œ ì¤„ì–´ë“¤ ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤.\n",
    "# ë‚˜ì¤‘ì— ë¡œë“œí•  ë•ŒëŠ” np.load()['X'] í˜•íƒœë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "\n",
    "save_path_compressed = os.path.join(save_dir, 'emg_data_compressed.npz')\n",
    "print(f\"3. ì••ì¶• ì €ì¥ ì‹œì‘ (ê¶Œì¥): {save_path_compressed} ...\")\n",
    "\n",
    "np.savez_compressed(save_path_compressed, X=X_combined, y=y_combined)\n",
    "\n",
    "print(\"ëª¨ë“  ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "113ea7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...\n",
      "ğŸ”„ ì›ë³¸ ë°ì´í„° í˜•íƒœ: (1803, 50040, 16)\n",
      "âœ… ë°ì´í„° í˜•íƒœ ìœ ì§€ë¨: (Time, Channels)\n",
      "ğŸ“Œ ìµœì¢… ì„¤ì • - Window Size(Time): 50040, Channels: 16\n",
      "WARNING:tensorflow:From c:\\Users\\dlrmflf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Channel_Temporal_Attention_EMG\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Channel_Temporal_Attention_EMG\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50040</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>) â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50040</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50040</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePoolâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">264</span> â”‚ global_average_pâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span> â”‚ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50040</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50040</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,200</span> â”‚ multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25020</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25020</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25020</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> â”‚ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d_1     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12510</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12510</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> â”‚ max_pooling1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12510</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)  â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> â”‚ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Softmax</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12510</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multiply_1          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12510</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ max_pooling1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)          â”‚                   â”‚            â”‚ softmax[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ multiply_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> â”‚ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)        â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,235</span> â”‚ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50040\u001b[0m, \u001b[38;5;34m16\u001b[0m) â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50040\u001b[0m, \u001b[38;5;34m32\u001b[0m) â”‚        \u001b[38;5;34m512\u001b[0m â”‚ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50040\u001b[0m, \u001b[38;5;34m32\u001b[0m) â”‚        \u001b[38;5;34m128\u001b[0m â”‚ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePoolâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         â”‚        \u001b[38;5;34m264\u001b[0m â”‚ global_average_pâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚        \u001b[38;5;34m288\u001b[0m â”‚ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ reshape (\u001b[38;5;33mReshape\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multiply (\u001b[38;5;33mMultiply\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50040\u001b[0m, \u001b[38;5;34m32\u001b[0m) â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50040\u001b[0m, \u001b[38;5;34m32\u001b[0m) â”‚      \u001b[38;5;34m7,200\u001b[0m â”‚ multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25020\u001b[0m, \u001b[38;5;34m32\u001b[0m) â”‚          \u001b[38;5;34m0\u001b[0m â”‚ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”‚ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25020\u001b[0m, \u001b[38;5;34m32\u001b[0m) â”‚          \u001b[38;5;34m0\u001b[0m â”‚ max_pooling1d[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25020\u001b[0m, \u001b[38;5;34m64\u001b[0m) â”‚      \u001b[38;5;34m6,208\u001b[0m â”‚ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d_1     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12510\u001b[0m, \u001b[38;5;34m64\u001b[0m) â”‚          \u001b[38;5;34m0\u001b[0m â”‚ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”‚ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12510\u001b[0m, \u001b[38;5;34m32\u001b[0m) â”‚      \u001b[38;5;34m2,080\u001b[0m â”‚ max_pooling1d_1[\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12510\u001b[0m, \u001b[38;5;34m1\u001b[0m)  â”‚         \u001b[38;5;34m33\u001b[0m â”‚ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ softmax (\u001b[38;5;33mSoftmax\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12510\u001b[0m, \u001b[38;5;34m1\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multiply_1          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12510\u001b[0m, \u001b[38;5;34m64\u001b[0m) â”‚          \u001b[38;5;34m0\u001b[0m â”‚ max_pooling1d_1[\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mMultiply\u001b[0m)          â”‚                   â”‚            â”‚ softmax[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda (\u001b[38;5;33mLambda\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ multiply_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚      \u001b[38;5;34m4,160\u001b[0m â”‚ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_5 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)        â”‚      \u001b[38;5;34m1,235\u001b[0m â”‚ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,108</span> (86.36 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m22,108\u001b[0m (86.36 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,044</span> (86.11 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m22,044\u001b[0m (86.11 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> (256.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m64\u001b[0m (256.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ í•™ìŠµ ì‹œì‘...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dlrmflf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”\u001b[0m \u001b[1m49s\u001b[0m 4s/step - accuracy: 0.0865 - loss: 2.9145"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 182\u001b[0m\n\u001b[0;32m    174\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    175\u001b[0m         train_gen,\n\u001b[0;32m    176\u001b[0m         validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val),\n\u001b[0;32m    177\u001b[0m         epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m    178\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks\n\u001b[0;32m    179\u001b[0m     )\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 182\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 174\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    167\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    168\u001b[0m     EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    169\u001b[0m     ModelCheckpoint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_channel_attention_model.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    170\u001b[0m     ReduceLROnPlateau(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    171\u001b[0m ]\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸš€ í•™ìŠµ ì‹œì‘...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 174\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dlrmflf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\dlrmflf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    376\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(begin_step)\n\u001b[1;32m--> 377\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\dlrmflf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    218\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    219\u001b[0m     ):\n\u001b[1;32m--> 220\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dlrmflf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\dlrmflf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\dlrmflf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dlrmflf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dlrmflf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\dlrmflf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\dlrmflf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\dlrmflf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\dlrmflf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import os, gc\n",
    "\n",
    "# ====================================================\n",
    "# 1. Balanced Data Generator (ìˆ˜ì • ì—†ìŒ)\n",
    "# ====================================================\n",
    "class BalancedDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, x, y, batch_size=32):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.classes = np.unique(y)\n",
    "        self.num_classes = len(self.classes)\n",
    "        self.class_indices = [np.where(y == c)[0] for c in self.classes]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bx, by = [], []\n",
    "        samples_per_class = self.batch_size // self.num_classes\n",
    "\n",
    "        for i, c in enumerate(self.classes):\n",
    "            # ìƒ˜í”Œì´ ë¶€ì¡±í•  ê²½ìš° replace=Trueë¡œ ì¤‘ë³µ í—ˆìš©\n",
    "            indices = np.random.choice(self.class_indices[i],\n",
    "                                       samples_per_class,\n",
    "                                       replace=True)\n",
    "            bx.append(self.x[indices])\n",
    "            by.append(self.y[indices])\n",
    "\n",
    "        bx = np.concatenate(bx)\n",
    "        by = np.concatenate(by)\n",
    "        p = np.random.permutation(len(bx))\n",
    "        return bx[p], by[p]\n",
    "\n",
    "# ====================================================\n",
    "# 2. Channel Attention (SE Block)\n",
    "# ====================================================\n",
    "def channel_attention(x, reduction=4): # reduction ë¹„ìœ¨ ì¡°ì •\n",
    "    # x shape: (Batch, Time, Channels)\n",
    "    channels = x.shape[-1] \n",
    "    \n",
    "    # GlobalAveragePooling1DëŠ” Time ì¶•ì„ í‰ê· ëƒ„ -> (Batch, Channels)\n",
    "    se = layers.GlobalAveragePooling1D()(x)\n",
    "    se = layers.Dense(max(1, channels // reduction), activation='relu')(se) # ìµœì†Œ 1 ë³´ì¥\n",
    "    se = layers.Dense(channels, activation='sigmoid')(se)\n",
    "    se = layers.Reshape((1, channels))(se) # (Batch, 1, Channels)\n",
    "\n",
    "    # Broadcasting: (Batch, Time, Channels) * (Batch, 1, Channels)\n",
    "    return layers.Multiply()([x, se])\n",
    "\n",
    "# ====================================================\n",
    "# 3. Channel-wise CNN + Attention Model (êµ¬ì¡° ê°œì„ )\n",
    "# ====================================================\n",
    "def build_channel_temporal_attention_model(num_classes, channels, window_size, d_model=64):\n",
    "    # ì…ë ¥: (Batch, Time, Channels) í˜•íƒœë¡œ ë°›ëŠ” ê²ƒì´ Keras í‘œì¤€ì— ë” ì í•©í•©ë‹ˆë‹¤.\n",
    "    # í•˜ì§€ë§Œ ê¸°ì¡´ ë°ì´í„°ê°€ (Channels, Time)ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì…ë ¥ ì •ì˜ëŠ” ìœ ì—°í•˜ê²Œ í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    # [ìˆ˜ì •] ì…ë ¥ í˜•íƒœë¥¼ ëª…í™•íˆ (Time, Channels)ë¡œ ê°€ì •í•˜ê³  ì„¤ê³„\n",
    "    # TimeDistributedë‚˜ Conv1DëŠ” ê¸°ë³¸ì ìœ¼ë¡œ (Batch, Time, Channels)ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.\n",
    "    inputs = Input(shape=(window_size, channels)) \n",
    "\n",
    "    # ===== Channel-wise CNN =====\n",
    "    # groups=channelsë¥¼ ì“°ë©´ ê° ì±„ë„ë³„ë¡œ ë…ë¦½ì ì¸ Conv ì—°ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤ (Depthwise Conv)\n",
    "    x = layers.Conv1D(\n",
    "        filters=channels * 2, # ì±„ë„ ìˆ˜ì˜ 2ë°°ë¡œ í™•ì¥\n",
    "        kernel_size=15,\n",
    "        padding=\"same\",\n",
    "        groups=channels,      # ğŸ”¥ ê° ì±„ë„ ë…ë¦½ ì—°ì‚°\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=regularizers.l2(0.01)\n",
    "    )(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # ===== Channel Attention =====\n",
    "    x = channel_attention(x)\n",
    "\n",
    "    # ===== Temporal Feature Extraction =====\n",
    "    x = layers.Conv1D(32, 7, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Conv1D(d_model, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    # ===== Temporal Attention =====\n",
    "    attn = layers.Dense(d_model // 2, activation=\"tanh\")(x)\n",
    "    attn_score = layers.Dense(1)(attn)\n",
    "    attn_weight = layers.Softmax(axis=1)(attn_score)\n",
    "\n",
    "    x = layers.Multiply()([x, attn_weight])\n",
    "    \n",
    "    # ì•ˆì „í•œ í•©ì‚° (Lambda ëŒ€ì‹  reduce_sum ì§ì ‘ ì‚¬ìš© ê°€ëŠ¥í•˜ì§€ë§Œ Lambdaë„ ë¬´ë°©)\n",
    "    x = layers.Lambda(lambda z: tf.reduce_sum(z, axis=1))(x)\n",
    "\n",
    "    # ===== Classifier =====\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return models.Model(inputs, outputs, name=\"Channel_Temporal_Attention_EMG\")\n",
    "\n",
    "# ====================================================\n",
    "# 4. Training (ì°¨ì› ìë™ ê°ì§€ ê¸°ëŠ¥ ì¶”ê°€)\n",
    "# ====================================================\n",
    "def train():\n",
    "    base_path = r'C:\\PJ_python\\Project-Gesture-classification-technique-based-on-multiple-EMG-datasets-main\\processed_data'\n",
    "\n",
    "    print(\"ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "    X = np.load(os.path.join(base_path, 'X_combined.npy')).astype(np.float32)\n",
    "    y = np.load(os.path.join(base_path, 'y_combined.npy'))\n",
    "\n",
    "    # [í•µì‹¬ ìˆ˜ì •] ì°¨ì› í™•ì¸ ë° ìë™ ì •ë ¬\n",
    "    print(f\"ğŸ”„ ì›ë³¸ ë°ì´í„° í˜•íƒœ: {X.shape}\")\n",
    "    n, dim1, dim2 = X.shape\n",
    "    \n",
    "    # ë³´í†µ EMG ë°ì´í„°ì—ì„œ (16, 500) ë˜ëŠ” (500, 16) ì¤‘ ì‘ì€ ìˆ«ìê°€ ì±„ë„ì…ë‹ˆë‹¤.\n",
    "    if dim1 < dim2:\n",
    "        channels, window_size = dim1, dim2\n",
    "        # ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ (Batch, Time, Channels) í˜•íƒœì¸ (N, 500, 16)ìœ¼ë¡œ ë³€ê²½ ê¶Œì¥\n",
    "        # í˜„ì¬ (N, 16, 500) -> (N, 500, 16)\n",
    "        X = np.transpose(X, (0, 2, 1))\n",
    "        print(f\"âœ… ë°ì´í„° ì „ì¹˜ ìˆ˜í–‰ë¨: (Channels, Time) -> (Time, Channels)\")\n",
    "    else:\n",
    "        window_size, channels = dim1, dim2\n",
    "        print(f\"âœ… ë°ì´í„° í˜•íƒœ ìœ ì§€ë¨: (Time, Channels)\")\n",
    "\n",
    "    print(f\"ğŸ“Œ ìµœì¢… ì„¤ì • - Window Size(Time): {window_size}, Channels: {channels}\")\n",
    "\n",
    "    # ===== Normalize =====\n",
    "    # (N, Time, Channels) í˜•íƒœ ê¸°ì¤€ ì •ê·œí™”\n",
    "    X = X.reshape(n, -1)\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X = X.reshape(n, window_size, channels) # ìˆœì„œ ì£¼ì˜\n",
    "\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    num_classes = len(np.unique(y))\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    train_gen = BalancedDataGenerator(X_train, y_train, batch_size=32)\n",
    "\n",
    "    # ëª¨ë¸ ìƒì„± ì‹œ ì˜¬ë°”ë¥¸ íŒŒë¼ë¯¸í„° ì „ë‹¬\n",
    "    model = build_channel_temporal_attention_model(\n",
    "        num_classes=num_classes,\n",
    "        channels=channels,       # 16ì´ì–´ì•¼ í•¨\n",
    "        window_size=window_size  # 50000 ë“± í° ìˆ«ìì—¬ì•¼ í•¨\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-3),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        ModelCheckpoint(\"best_channel_attention_model.keras\", save_best_only=True),\n",
    "        ReduceLROnPlateau(patience=3, factor=0.5, verbose=1)\n",
    "    ]\n",
    "\n",
    "    print(\"ğŸš€ í•™ìŠµ ì‹œì‘...\")\n",
    "    model.fit(\n",
    "        train_gen,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=100,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b40d946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Device: cuda\n",
      "ğŸ“‚ Loading Data...\n",
      "âš–ï¸ Scaling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dlrmflf\\AppData\\Local\\Temp\\ipykernel_28196\\2955535996.py:179: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Start Training (OneCycleLR + TimeMasking)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dlrmflf\\AppData\\Local\\Temp\\ipykernel_28196\\2955535996.py:195: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] | LR: 0.000043 | Train Loss: 2.9029 Acc: 0.0908 | Val Loss: 2.7967 Acc: 0.1136\n",
      "ğŸ‰ Best Model Saved! (Acc: 0.1136)\n",
      "Epoch [2/100] | LR: 0.000051 | Train Loss: 2.8385 Acc: 0.1061 | Val Loss: 2.6876 Acc: 0.1247\n",
      "ğŸ‰ Best Model Saved! (Acc: 0.1247)\n",
      "Epoch [3/100] | LR: 0.000064 | Train Loss: 2.7974 Acc: 0.1033 | Val Loss: 2.6912 Acc: 0.2050\n",
      "ğŸ‰ Best Model Saved! (Acc: 0.2050)\n",
      "Epoch [4/100] | LR: 0.000082 | Train Loss: 2.7772 Acc: 0.1123 | Val Loss: 2.5748 Acc: 0.1634\n",
      "Epoch [5/100] | LR: 0.000104 | Train Loss: 2.7535 Acc: 0.1276 | Val Loss: 2.5659 Acc: 0.2382\n",
      "ğŸ‰ Best Model Saved! (Acc: 0.2382)\n",
      "Epoch [6/100] | LR: 0.000132 | Train Loss: 2.7182 Acc: 0.1484 | Val Loss: 2.6473 Acc: 0.1191\n",
      "Epoch [7/100] | LR: 0.000163 | Train Loss: 2.6858 Acc: 0.1789 | Val Loss: 2.5085 Acc: 0.3241\n",
      "ğŸ‰ Best Model Saved! (Acc: 0.3241)\n",
      "Epoch [8/100] | LR: 0.000199 | Train Loss: 2.6508 Acc: 0.1852 | Val Loss: 2.7119 Acc: 0.1856\n",
      "Epoch [9/100] | LR: 0.000238 | Train Loss: 2.6532 Acc: 0.1886 | Val Loss: 2.5388 Acc: 0.2078\n",
      "Epoch [10/100] | LR: 0.000280 | Train Loss: 2.6259 Acc: 0.1983 | Val Loss: 2.4319 Acc: 0.2964\n",
      "Epoch [11/100] | LR: 0.000325 | Train Loss: 2.5700 Acc: 0.2490 | Val Loss: 2.5664 Acc: 0.2548\n",
      "Epoch [12/100] | LR: 0.000372 | Train Loss: 2.5508 Acc: 0.2705 | Val Loss: 2.3486 Acc: 0.3324\n",
      "ğŸ‰ Best Model Saved! (Acc: 0.3324)\n",
      "Epoch [13/100] | LR: 0.000421 | Train Loss: 2.5207 Acc: 0.2788 | Val Loss: 2.2686 Acc: 0.4155\n",
      "ğŸ‰ Best Model Saved! (Acc: 0.4155)\n",
      "Epoch [14/100] | LR: 0.000470 | Train Loss: 2.4274 Acc: 0.3100 | Val Loss: 2.1872 Acc: 0.4404\n",
      "ğŸ‰ Best Model Saved! (Acc: 0.4404)\n",
      "Epoch [15/100] | LR: 0.000521 | Train Loss: 2.3402 Acc: 0.3454 | Val Loss: 2.2412 Acc: 0.5346\n",
      "ğŸ‰ Best Model Saved! (Acc: 0.5346)\n",
      "Epoch [16/100] | LR: 0.000571 | Train Loss: 2.2750 Acc: 0.3953 | Val Loss: 2.7079 Acc: 0.2992\n",
      "Epoch [17/100] | LR: 0.000620 | Train Loss: 2.1803 Acc: 0.4362 | Val Loss: 2.8361 Acc: 0.2271\n",
      "Epoch [18/100] | LR: 0.000669 | Train Loss: 2.1265 Acc: 0.4785 | Val Loss: 2.6275 Acc: 0.1939\n",
      "Epoch [19/100] | LR: 0.000716 | Train Loss: 2.0550 Acc: 0.5354 | Val Loss: 2.4871 Acc: 0.2548\n",
      "Epoch [20/100] | LR: 0.000761 | Train Loss: 1.9786 Acc: 0.5465 | Val Loss: 2.2884 Acc: 0.3850\n",
      "Epoch [21/100] | LR: 0.000803 | Train Loss: 1.9940 Acc: 0.5687 | Val Loss: 2.4143 Acc: 0.3463\n",
      "Epoch [22/100] | LR: 0.000842 | Train Loss: 1.9171 Acc: 0.5915 | Val Loss: 1.8919 Acc: 0.6066\n",
      "ğŸ‰ Best Model Saved! (Acc: 0.6066)\n",
      "Epoch [23/100] | LR: 0.000877 | Train Loss: 1.9182 Acc: 0.6137 | Val Loss: 2.2338 Acc: 0.3657\n",
      "Epoch [24/100] | LR: 0.000909 | Train Loss: 1.8827 Acc: 0.6096 | Val Loss: 2.1002 Acc: 0.4349\n",
      "Epoch [25/100] | LR: 0.000936 | Train Loss: 1.9024 Acc: 0.5943 | Val Loss: 2.1818 Acc: 0.4349\n",
      "Epoch [26/100] | LR: 0.000959 | Train Loss: 1.8572 Acc: 0.6380 | Val Loss: 2.8155 Acc: 0.2825\n",
      "Epoch [27/100] | LR: 0.000977 | Train Loss: 1.7885 Acc: 0.6734 | Val Loss: 1.8289 Acc: 0.6150\n",
      "ğŸ‰ Best Model Saved! (Acc: 0.6150)\n",
      "Epoch [28/100] | LR: 0.000990 | Train Loss: 1.7650 Acc: 0.6838 | Val Loss: 1.9441 Acc: 0.5596\n",
      "Epoch [29/100] | LR: 0.000997 | Train Loss: 1.7993 Acc: 0.6560 | Val Loss: 2.9511 Acc: 0.2327\n",
      "Epoch [30/100] | LR: 0.001000 | Train Loss: 1.7361 Acc: 0.7080 | Val Loss: 1.8685 Acc: 0.6260\n",
      "ğŸ‰ Best Model Saved! (Acc: 0.6260)\n",
      "Epoch [31/100] | LR: 0.000999 | Train Loss: 1.7444 Acc: 0.6872 | Val Loss: 1.9751 Acc: 0.5734\n",
      "Epoch [32/100] | LR: 0.000998 | Train Loss: 1.6888 Acc: 0.7226 | Val Loss: 2.8267 Acc: 0.3269\n",
      "Epoch [33/100] | LR: 0.000995 | Train Loss: 1.6579 Acc: 0.7573 | Val Loss: 2.3489 Acc: 0.3740\n",
      "Epoch [34/100] | LR: 0.000992 | Train Loss: 1.6525 Acc: 0.7358 | Val Loss: 2.2553 Acc: 0.3380\n",
      "Epoch [35/100] | LR: 0.000987 | Train Loss: 1.6089 Acc: 0.7656 | Val Loss: 2.2597 Acc: 0.4072\n",
      "Epoch [36/100] | LR: 0.000982 | Train Loss: 1.6294 Acc: 0.7497 | Val Loss: 1.9523 Acc: 0.5762\n",
      "Epoch [37/100] | LR: 0.000975 | Train Loss: 1.6544 Acc: 0.7455 | Val Loss: 3.0518 Acc: 0.2825\n",
      "Epoch [38/100] | LR: 0.000968 | Train Loss: 1.6370 Acc: 0.7587 | Val Loss: 1.8934 Acc: 0.5845\n",
      "Epoch [39/100] | LR: 0.000960 | Train Loss: 1.5271 Acc: 0.8218 | Val Loss: 2.4395 Acc: 0.3186\n",
      "Epoch [40/100] | LR: 0.000950 | Train Loss: 1.5666 Acc: 0.7933 | Val Loss: 1.8085 Acc: 0.6759\n",
      "ğŸ‰ Best Model Saved! (Acc: 0.6759)\n",
      "Epoch [41/100] | LR: 0.000940 | Train Loss: 1.5659 Acc: 0.8121 | Val Loss: 2.3825 Acc: 0.3989\n",
      "Epoch [42/100] | LR: 0.000929 | Train Loss: 1.5257 Acc: 0.8308 | Val Loss: 2.8719 Acc: 0.2299\n",
      "Epoch [43/100] | LR: 0.000917 | Train Loss: 1.5725 Acc: 0.8051 | Val Loss: 1.9143 Acc: 0.6233\n",
      "Epoch [44/100] | LR: 0.000904 | Train Loss: 1.5326 Acc: 0.8183 | Val Loss: 1.9955 Acc: 0.5235\n",
      "Epoch [45/100] | LR: 0.000891 | Train Loss: 1.4799 Acc: 0.8481 | Val Loss: 2.0783 Acc: 0.5734\n",
      "Epoch [46/100] | LR: 0.000876 | Train Loss: 1.5162 Acc: 0.8301 | Val Loss: 1.9847 Acc: 0.5928\n",
      "Epoch [47/100] | LR: 0.000861 | Train Loss: 1.5007 Acc: 0.8301 | Val Loss: 2.9801 Acc: 0.3075\n",
      "Epoch [48/100] | LR: 0.000845 | Train Loss: 1.5039 Acc: 0.8391 | Val Loss: 2.1097 Acc: 0.5263\n",
      "Epoch [49/100] | LR: 0.000829 | Train Loss: 1.4929 Acc: 0.8447 | Val Loss: 2.0092 Acc: 0.5762\n",
      "Epoch [50/100] | LR: 0.000811 | Train Loss: 1.4784 Acc: 0.8460 | Val Loss: 2.0235 Acc: 0.5457\n",
      "Epoch [51/100] | LR: 0.000793 | Train Loss: 1.4622 Acc: 0.8544 | Val Loss: 2.0894 Acc: 0.4848\n",
      "Epoch [52/100] | LR: 0.000775 | Train Loss: 1.4420 Acc: 0.8696 | Val Loss: 1.8246 Acc: 0.6288\n",
      "Epoch [53/100] | LR: 0.000756 | Train Loss: 1.4589 Acc: 0.8599 | Val Loss: 2.4320 Acc: 0.4017\n",
      "Epoch [54/100] | LR: 0.000737 | Train Loss: 1.4568 Acc: 0.8537 | Val Loss: 2.5038 Acc: 0.4460\n",
      "Epoch [55/100] | LR: 0.000717 | Train Loss: 1.4797 Acc: 0.8558 | Val Loss: 2.1331 Acc: 0.4765\n",
      "Epoch [56/100] | LR: 0.000696 | Train Loss: 1.4733 Acc: 0.8634 | Val Loss: 1.8117 Acc: 0.6371\n",
      "Epoch [57/100] | LR: 0.000675 | Train Loss: 1.4427 Acc: 0.8710 | Val Loss: 1.7987 Acc: 0.6870\n",
      "ğŸ‰ Best Model Saved! (Acc: 0.6870)\n",
      "Epoch [58/100] | LR: 0.000654 | Train Loss: 1.3997 Acc: 0.8877 | Val Loss: 1.7755 Acc: 0.6759\n",
      "Epoch [59/100] | LR: 0.000633 | Train Loss: 1.3940 Acc: 0.9071 | Val Loss: 1.7998 Acc: 0.6399\n",
      "Epoch [60/100] | LR: 0.000611 | Train Loss: 1.4247 Acc: 0.8752 | Val Loss: 2.0717 Acc: 0.5263\n",
      "Epoch [61/100] | LR: 0.000589 | Train Loss: 1.4075 Acc: 0.8877 | Val Loss: 1.8000 Acc: 0.6648\n",
      "Epoch [62/100] | LR: 0.000567 | Train Loss: 1.4145 Acc: 0.8849 | Val Loss: 2.2373 Acc: 0.4709\n",
      "Epoch [63/100] | LR: 0.000544 | Train Loss: 1.4145 Acc: 0.8717 | Val Loss: 2.3293 Acc: 0.4737\n",
      "Epoch [64/100] | LR: 0.000522 | Train Loss: 1.3884 Acc: 0.9008 | Val Loss: 1.9542 Acc: 0.5346\n",
      "Epoch [65/100] | LR: 0.000500 | Train Loss: 1.4014 Acc: 0.8981 | Val Loss: 1.8539 Acc: 0.6039\n",
      "Epoch [66/100] | LR: 0.000477 | Train Loss: 1.3766 Acc: 0.9071 | Val Loss: 2.0643 Acc: 0.5596\n",
      "Epoch [67/100] | LR: 0.000455 | Train Loss: 1.4332 Acc: 0.8731 | Val Loss: 1.6874 Acc: 0.7313\n",
      "ğŸ‰ Best Model Saved! (Acc: 0.7313)\n",
      "Epoch [68/100] | LR: 0.000432 | Train Loss: 1.3735 Acc: 0.9147 | Val Loss: 1.7467 Acc: 0.6925\n",
      "Epoch [69/100] | LR: 0.000410 | Train Loss: 1.3939 Acc: 0.8974 | Val Loss: 1.7341 Acc: 0.7285\n",
      "Epoch [70/100] | LR: 0.000388 | Train Loss: 1.3800 Acc: 0.9015 | Val Loss: 1.7429 Acc: 0.6593\n",
      "Epoch [71/100] | LR: 0.000367 | Train Loss: 1.3723 Acc: 0.9008 | Val Loss: 1.7114 Acc: 0.6953\n",
      "Epoch [72/100] | LR: 0.000345 | Train Loss: 1.3893 Acc: 0.8960 | Val Loss: 1.8520 Acc: 0.6094\n",
      "Epoch [73/100] | LR: 0.000324 | Train Loss: 1.3710 Acc: 0.9008 | Val Loss: 1.6887 Acc: 0.7175\n",
      "Epoch [74/100] | LR: 0.000303 | Train Loss: 1.3522 Acc: 0.9161 | Val Loss: 1.7090 Acc: 0.7064\n",
      "Epoch [75/100] | LR: 0.000283 | Train Loss: 1.3459 Acc: 0.9133 | Val Loss: 1.7594 Acc: 0.6371\n",
      "Epoch [76/100] | LR: 0.000263 | Train Loss: 1.3297 Acc: 0.9230 | Val Loss: 1.9207 Acc: 0.5983\n",
      "Epoch [77/100] | LR: 0.000243 | Train Loss: 1.3294 Acc: 0.9175 | Val Loss: 1.8862 Acc: 0.6011\n",
      "Epoch [78/100] | LR: 0.000224 | Train Loss: 1.3138 Acc: 0.9272 | Val Loss: 1.6490 Acc: 0.7479\n",
      "ğŸ‰ Best Model Saved! (Acc: 0.7479)\n",
      "Epoch [79/100] | LR: 0.000206 | Train Loss: 1.3502 Acc: 0.9126 | Val Loss: 1.6352 Acc: 0.7479\n",
      "Epoch [80/100] | LR: 0.000188 | Train Loss: 1.3657 Acc: 0.9008 | Val Loss: 1.6453 Acc: 0.7285\n",
      "Epoch [81/100] | LR: 0.000171 | Train Loss: 1.3301 Acc: 0.9272 | Val Loss: 1.7124 Acc: 0.7036\n",
      "Epoch [82/100] | LR: 0.000154 | Train Loss: 1.3115 Acc: 0.9355 | Val Loss: 1.6474 Acc: 0.7313\n",
      "Epoch [83/100] | LR: 0.000138 | Train Loss: 1.3234 Acc: 0.9251 | Val Loss: 1.6435 Acc: 0.7341\n",
      "Epoch [84/100] | LR: 0.000123 | Train Loss: 1.3163 Acc: 0.9307 | Val Loss: 1.6557 Acc: 0.7424\n",
      "Epoch [85/100] | LR: 0.000109 | Train Loss: 1.3384 Acc: 0.9175 | Val Loss: 1.6295 Acc: 0.7258\n",
      "Epoch [86/100] | LR: 0.000095 | Train Loss: 1.3236 Acc: 0.9272 | Val Loss: 1.6273 Acc: 0.7507\n",
      "ğŸ‰ Best Model Saved! (Acc: 0.7507)\n",
      "Epoch [87/100] | LR: 0.000082 | Train Loss: 1.3260 Acc: 0.9202 | Val Loss: 1.6155 Acc: 0.7562\n",
      "ğŸ‰ Best Model Saved! (Acc: 0.7562)\n",
      "Epoch [88/100] | LR: 0.000071 | Train Loss: 1.3253 Acc: 0.9216 | Val Loss: 1.6052 Acc: 0.7368\n",
      "Epoch [89/100] | LR: 0.000059 | Train Loss: 1.3347 Acc: 0.9244 | Val Loss: 1.6246 Acc: 0.7452\n",
      "Epoch [90/100] | LR: 0.000049 | Train Loss: 1.3130 Acc: 0.9279 | Val Loss: 1.6138 Acc: 0.7562\n",
      "Epoch [91/100] | LR: 0.000040 | Train Loss: 1.3162 Acc: 0.9307 | Val Loss: 1.6175 Acc: 0.7618\n",
      "ğŸ‰ Best Model Saved! (Acc: 0.7618)\n",
      "Epoch [92/100] | LR: 0.000032 | Train Loss: 1.3388 Acc: 0.9265 | Val Loss: 1.6179 Acc: 0.7618\n",
      "Epoch [93/100] | LR: 0.000024 | Train Loss: 1.3150 Acc: 0.9327 | Val Loss: 1.7010 Acc: 0.7202\n",
      "Epoch [94/100] | LR: 0.000018 | Train Loss: 1.3115 Acc: 0.9320 | Val Loss: 1.6214 Acc: 0.7562\n",
      "Epoch [95/100] | LR: 0.000012 | Train Loss: 1.3385 Acc: 0.9230 | Val Loss: 1.6318 Acc: 0.7507\n",
      "Epoch [96/100] | LR: 0.000008 | Train Loss: 1.3584 Acc: 0.9209 | Val Loss: 1.6162 Acc: 0.7479\n",
      "Epoch [97/100] | LR: 0.000004 | Train Loss: 1.3302 Acc: 0.9293 | Val Loss: 1.6165 Acc: 0.7618\n",
      "Epoch [98/100] | LR: 0.000002 | Train Loss: 1.3265 Acc: 0.9286 | Val Loss: 1.6150 Acc: 0.7507\n",
      "Epoch [99/100] | LR: 0.000000 | Train Loss: 1.3241 Acc: 0.9258 | Val Loss: 1.6295 Acc: 0.7507\n",
      "Epoch [100/100] | LR: 0.000000 | Train Loss: 1.3544 Acc: 0.9161 | Val Loss: 1.6344 Acc: 0.7424\n",
      "âœ… Training Finished. Best Validation Accuracy: 0.7618\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Device Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸš€ Device: {device}\")\n",
    "\n",
    "# ====================================================\n",
    "# 1. Dataset (ê¸°ë³¸ ì¦ê°•ë§Œ ìœ ì§€)\n",
    "# ====================================================\n",
    "class EMGDataset(Dataset):\n",
    "    def __init__(self, X, y, mode='train'):\n",
    "        self.mode = mode\n",
    "        if X.shape[1] > X.shape[2]: \n",
    "            X = X.transpose(0, 2, 1)\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def augment(self, x):\n",
    "        # ê³¼ë„í•œ ë…¸ì´ì¦ˆëŠ” ì œê±°í•˜ê³  ê¸°ë³¸ ìŠ¤ì¼€ì¼ë§ë§Œ ì ìš©\n",
    "        if torch.rand(1) < 0.5:\n",
    "            scale = 1.0 + (torch.rand(1) * 0.1 - 0.05) # Â±5%\n",
    "            x = x * scale.to(x.device)\n",
    "        return x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        if self.mode == 'train':\n",
    "            x = self.augment(x)\n",
    "        return x, y\n",
    "\n",
    "# ====================================================\n",
    "# 2. [NEW] Multi-Scale Inception Module\n",
    "# ====================================================\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        \n",
    "        # Branch 1: ì‘ì€ íŒ¨í„´ (Kernel 15)\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=15, padding=7),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Branch 2: ì¤‘ê°„ íŒ¨í„´ (Kernel 51)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=51, padding=25),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Branch 3: ê¸´ íŒ¨í„´ (Kernel 101) - ìˆ²ì„ ë³´ëŠ” ì‹œì•¼\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=101, padding=50),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Branch 4: Max Pooling (ì¤‘ìš”í•œ íŠ¹ì§• ê°•ì¡°)\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b1 = self.branch1(x)\n",
    "        b2 = self.branch2(x)\n",
    "        b3 = self.branch3(x)\n",
    "        b4 = self.branch4(x)\n",
    "        return torch.cat([b1, b2, b3, b4], dim=1) # ì±„ë„ ë°©í–¥ ì—°ê²°\n",
    "\n",
    "# ====================================================\n",
    "# 3. Model Architecture (Inception + Attention)\n",
    "# ====================================================\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.query = nn.Linear(d_model, d_model // 2)\n",
    "        self.score = nn.Linear(d_model // 2, 1)\n",
    "    def forward(self, x):\n",
    "        x_perm = x.permute(0, 2, 1) \n",
    "        attn = torch.tanh(self.query(x_perm))\n",
    "        scores = self.score(attn)\n",
    "        weights = torch.softmax(scores, dim=1) \n",
    "        context = x_perm * weights\n",
    "        context = torch.sum(context, dim=1)\n",
    "        return context\n",
    "\n",
    "class MultiScale_EMG_Model(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels, window_size, d_model=128): \n",
    "        super(MultiScale_EMG_Model, self).__init__()\n",
    "        \n",
    "        # 1. ì´ˆê¸° ì••ì¶• (ë°ì´í„° ê¸¸ì´ê°€ ë„ˆë¬´ ê¸¸ì–´ì„œ Stride=4ë¡œ ì¤„ì„)\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 64, kernel_size=7, stride=4, padding=3),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2) # 50,000 -> 12,500 -> 6,250\n",
    "        )\n",
    "        \n",
    "        # 2. Inception Blocks (ë‹¤ì–‘í•œ ì‹œì•¼ë¡œ íŠ¹ì§• ì¶”ì¶œ)\n",
    "        # 4ê°œ ë¸Œëœì¹˜ê°€ í•©ì³ì§€ë¯€ë¡œ ì¶œë ¥ ì±„ë„ì€ out_channels * 4\n",
    "        self.inception1 = InceptionBlock(64, 32) # Output: 32*4 = 128 channels\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.inception2 = InceptionBlock(128, 64) # Output: 64*4 = 256 channels\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        # 3. Feature Reduction\n",
    "        self.conv_reduce = nn.Conv1d(256, d_model, kernel_size=1)\n",
    "        self.bn_reduce = nn.BatchNorm1d(d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 4. Attention & Classifier\n",
    "        self.temp_attn = TemporalAttention(d_model)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.inception1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.inception2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.conv_reduce(x)\n",
    "        x = self.bn_reduce(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.temp_attn(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ====================================================\n",
    "# 4. Training Loop (Unlock Performance Setting)\n",
    "# ====================================================\n",
    "def train_model():\n",
    "    base_path = r'C:\\PJ_python\\Project-Gesture-classification-technique-based-on-multiple-EMG-datasets-main\\processed_data'\n",
    "    \n",
    "    print(\"ğŸ“‚ Loading Data...\")\n",
    "    try:\n",
    "        X = np.load(os.path.join(base_path, 'X_combined.npy')).astype(np.float32)\n",
    "        y = np.load(os.path.join(base_path, 'y_combined.npy'))\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        return\n",
    "\n",
    "    n, dim1, dim2 = X.shape\n",
    "    if dim1 < dim2: \n",
    "        channels, window_size = dim1, dim2\n",
    "    else: \n",
    "        channels, window_size = dim2, dim1\n",
    "        X = np.transpose(X, (0, 2, 1))\n",
    "\n",
    "    print(\"âš–ï¸ Scaling...\")\n",
    "    X_perm = np.transpose(X, (0, 2, 1)).reshape(-1, channels)\n",
    "    scaler = StandardScaler(copy=False)\n",
    "    X_perm = scaler.fit_transform(X_perm)\n",
    "    X = np.transpose(X_perm.reshape(n, window_size, channels), (0, 2, 1))\n",
    "    del X_perm; gc.collect()\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y)\n",
    "    num_classes = len(np.unique(y_enc))\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y_enc, test_size=0.2, stratify=y_enc, random_state=42)\n",
    "    \n",
    "    train_dataset = EMGDataset(X_train, y_train, mode='train')\n",
    "    val_dataset = EMGDataset(X_val, y_val, mode='val')\n",
    "    \n",
    "    # Sampler ìœ ì§€\n",
    "    class_counts = np.bincount(y_train)\n",
    "    class_weights = 1. / class_counts\n",
    "    sample_weights = class_weights[y_train]\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    \n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    # [ë³€ê²½] Multi-Scale Model ì‚¬ìš©\n",
    "    model = MultiScale_EMG_Model(num_classes, channels, window_size, d_model=128).to(device)\n",
    "    \n",
    "    # Label Smoothing ì•½í•˜ê²Œ ì ìš©\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "    # Optimizer: ê·œì œë¥¼ ë‹¤ì‹œ í’€ì–´ì„œ(Weight Decay=0) ì„±ëŠ¥ í•´ì œ\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    num_epochs = 100\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    print(\"\\nğŸš€ Start Training (Multi-Scale Inception)...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = correct / total\n",
    "        \n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | LR: {current_lr:.6f} | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_emg_inception.pth')\n",
    "            print(f\"ğŸ‰ Best Model Saved! (Acc: {best_acc:.4f})\")\n",
    "            \n",
    "    print(f\"âœ… Training Finished. Best Validation Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90fc0b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Device: cuda\n",
      "ğŸ“‚ ë°ì´í„° ì •ë³´ë¥¼ ë‹¤ì‹œ ì½ì–´ì˜µë‹ˆë‹¤...\n",
      "ğŸ“Œ ê°ì§€ëœ ì„¤ì • - Channels: 16, Window: 50040\n",
      "âš–ï¸ Scaling ì ìš© ì¤‘...\n",
      "ğŸ“Œ í´ë˜ìŠ¤ ê°œìˆ˜: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dlrmflf\\AppData\\Local\\Temp\\ipykernel_28196\\2199228329.py:173: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Best Model ë¡œë“œ ì™„ë£Œ.\n",
      "\n",
      "ğŸš€ TTA(Test Time Augmentation) í‰ê°€ ì‹œì‘...\n",
      "âœ¨ TTA ì ìš© ìµœì¢… ì •í™•ë„: 76.18%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ====================================================\n",
    "# 1. ì„¤ì • ë° ì¥ì¹˜ í™•ì¸\n",
    "# ====================================================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸš€ Device: {device}\")\n",
    "\n",
    "# ====================================================\n",
    "# 2. ëª¨ë¸ êµ¬ì¡° ì •ì˜ (ì €ì¥ëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ í•„ìš”)\n",
    "# ====================================================\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        self.branch1 = nn.Sequential(nn.Conv1d(in_channels, out_channels, 15, padding=7), nn.BatchNorm1d(out_channels), nn.ReLU())\n",
    "        self.branch2 = nn.Sequential(nn.Conv1d(in_channels, out_channels, 51, padding=25), nn.BatchNorm1d(out_channels), nn.ReLU())\n",
    "        self.branch3 = nn.Sequential(nn.Conv1d(in_channels, out_channels, 101, padding=50), nn.BatchNorm1d(out_channels), nn.ReLU())\n",
    "        self.branch4 = nn.Sequential(nn.MaxPool1d(3, 1, 1), nn.Conv1d(in_channels, out_channels, 1), nn.BatchNorm1d(out_channels), nn.ReLU())\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], dim=1)\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.query = nn.Linear(d_model, d_model // 2)\n",
    "        self.score = nn.Linear(d_model // 2, 1)\n",
    "    def forward(self, x):\n",
    "        x_perm = x.permute(0, 2, 1) \n",
    "        attn = torch.tanh(self.query(x_perm))\n",
    "        scores = self.score(attn)\n",
    "        weights = torch.softmax(scores, dim=1) \n",
    "        context = x_perm * weights\n",
    "        context = torch.sum(context, dim=1)\n",
    "        return context\n",
    "\n",
    "class MultiScale_EMG_Model(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels, window_size, d_model=128): \n",
    "        super(MultiScale_EMG_Model, self).__init__()\n",
    "        self.stem = nn.Sequential(nn.Conv1d(input_channels, 64, 7, stride=4, padding=3), nn.BatchNorm1d(64), nn.ReLU(), nn.MaxPool1d(2))\n",
    "        self.inception1 = InceptionBlock(64, 32)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.inception2 = InceptionBlock(128, 64)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.conv_reduce = nn.Conv1d(256, d_model, 1)\n",
    "        self.bn_reduce = nn.BatchNorm1d(d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.temp_attn = TemporalAttention(d_model)\n",
    "        self.classifier = nn.Sequential(nn.Linear(d_model, 256), nn.ReLU(), nn.Dropout(0.4), nn.Linear(256, num_classes))\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.inception1(x); x = self.pool1(x); x = self.dropout1(x)\n",
    "        x = self.inception2(x); x = self.pool2(x); x = self.dropout2(x)\n",
    "        x = self.conv_reduce(x); x = self.bn_reduce(x); x = self.relu(x)\n",
    "        x = self.temp_attn(x); x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class EMGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # í‰ê°€ì‹œì—ëŠ” Transposeê°€ ì´ë¯¸ ë˜ì–´ìˆë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜ í™•ì¸ í•„ìš”\n",
    "        # ì—¬ê¸°ì„œëŠ” DataLoaderì— ë„£ê¸° ì „ ë¯¸ë¦¬ ì²˜ë¦¬ëœ Xë¥¼ ë°›ìŒ\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ====================================================\n",
    "# 3. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (ë³€ìˆ˜ ë³µì› ê³¼ì •)\n",
    "# ====================================================\n",
    "def load_data_and_prepare_loader():\n",
    "    base_path = r'C:\\PJ_python\\Project-Gesture-classification-technique-based-on-multiple-EMG-datasets-main\\processed_data'\n",
    "    \n",
    "    print(\"ğŸ“‚ ë°ì´í„° ì •ë³´ë¥¼ ë‹¤ì‹œ ì½ì–´ì˜µë‹ˆë‹¤...\")\n",
    "    try:\n",
    "        # ë°ì´í„° ë¡œë“œ\n",
    "        X = np.load(os.path.join(base_path, 'X_combined.npy')).astype(np.float32)\n",
    "        y = np.load(os.path.join(base_path, 'y_combined.npy'))\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # ì°¨ì› ì •ë³´ ë³µì›\n",
    "    n, dim1, dim2 = X.shape\n",
    "    if dim1 < dim2: \n",
    "        channels, window_size = dim1, dim2\n",
    "    else: \n",
    "        channels, window_size = dim2, dim1\n",
    "        X = np.transpose(X, (0, 2, 1))\n",
    "\n",
    "    print(f\"ğŸ“Œ ê°ì§€ëœ ì„¤ì • - Channels: {channels}, Window: {window_size}\")\n",
    "\n",
    "    # ìŠ¤ì¼€ì¼ë§ (í•™ìŠµ ë•Œì™€ ë™ì¼í•˜ê²Œ ì ìš©í•´ì•¼ í•¨)\n",
    "    print(\"âš–ï¸ Scaling ì ìš© ì¤‘...\")\n",
    "    X_perm = np.transpose(X, (0, 2, 1)).reshape(-1, channels)\n",
    "    scaler = StandardScaler(copy=False)\n",
    "    X_perm = scaler.fit_transform(X_perm)\n",
    "    X = np.transpose(X_perm.reshape(n, window_size, channels), (0, 2, 1))\n",
    "    \n",
    "    # ë¼ë²¨ë§\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y)\n",
    "    num_classes = len(np.unique(y_enc))\n",
    "    print(f\"ğŸ“Œ í´ë˜ìŠ¤ ê°œìˆ˜: {num_classes}\")\n",
    "\n",
    "    # Validation Set ë¶„ë¦¬ (í•™ìŠµ ë•Œì™€ ë™ì¼í•œ seed=42 ì‚¬ìš© í•„ìˆ˜)\n",
    "    _, X_val, _, y_val = train_test_split(X, y_enc, test_size=0.2, stratify=y_enc, random_state=42)\n",
    "    \n",
    "    # DataLoader ìƒì„±\n",
    "    val_dataset = EMGDataset(X_val, y_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    return val_loader, num_classes, channels, window_size\n",
    "\n",
    "# ====================================================\n",
    "# 4. TTA í‰ê°€ ì‹¤í–‰\n",
    "# ====================================================\n",
    "def evaluate_with_tta(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"\\nğŸš€ TTA(Test Time Augmentation) í‰ê°€ ì‹œì‘...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # 1. ì›ë³¸\n",
    "            output1 = model(inputs)\n",
    "            # 2. í™•ëŒ€ (5%)\n",
    "            output2 = model(inputs * 1.05)\n",
    "            # 3. ì¶•ì†Œ (5%)\n",
    "            output3 = model(inputs * 0.95)\n",
    "            \n",
    "            # Soft Voting\n",
    "            final_output = (output1 + output2 + output3) / 3.0\n",
    "            _, preds = torch.max(final_output, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"âœ¨ TTA ì ìš© ìµœì¢… ì •í™•ë„: {acc*100:.2f}%\")\n",
    "    return all_labels, all_preds\n",
    "\n",
    "# --- ë©”ì¸ ì‹¤í–‰ íë¦„ ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. ë°ì´í„° ë° ë³€ìˆ˜ ì¤€ë¹„\n",
    "    val_loader, num_classes, channels, window_size = load_data_and_prepare_loader()\n",
    "    \n",
    "    if val_loader is not None:\n",
    "        # 2. ëª¨ë¸ ì´ˆê¸°í™”\n",
    "        model = MultiScale_EMG_Model(num_classes, channels, window_size, d_model=128).to(device)\n",
    "        \n",
    "        # 3. ê°€ì¤‘ì¹˜ ë¡œë“œ (ê²½ë¡œ í™•ì¸ í•„ìˆ˜)\n",
    "        # best_emg_inception.pth íŒŒì¼ì´ í˜„ì¬ í´ë”ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "        model_path = 'best_emg_onecycle.pth' \n",
    "        \n",
    "        if os.path.exists(model_path):\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            print(\"âœ… Best Model ë¡œë“œ ì™„ë£Œ.\")\n",
    "            \n",
    "            # 4. í‰ê°€ ë° ì‹œê°í™”\n",
    "            y_true, y_pred = evaluate_with_tta(model, val_loader, device)\n",
    "            \n",
    "            # Confusion Matrix\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title(f'Confusion Matrix (TTA Accuracy: {accuracy_score(y_true, y_pred)*100:.2f}%)')\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"âš ï¸ ëª¨ë¸ íŒŒì¼({model_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645f5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
