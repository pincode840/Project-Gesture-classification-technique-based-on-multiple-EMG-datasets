{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "688ab5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9059e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ninapro 데이터셋 불러오기\n",
    "ninapro_df = pd.DataFrame()\n",
    "for i in range (1,11):\n",
    "    adress = f\"ninapro_db5/s{i}/S{i}_E2_A1\"\n",
    "    filename = adress\n",
    "    mat = sio.loadmat(filename)\n",
    "    emg = mat['emg']\n",
    "    Restimulus = mat['restimulus']\n",
    "    rerepetition = mat['rerepetition']\n",
    "    df_emg = pd.DataFrame(emg)\n",
    "    df_Restimulus = pd.DataFrame(Restimulus)\n",
    "#    df_rerepetition = pd.DataFrame(rerepetition)\n",
    "    df = pd.concat([df_emg, df_Restimulus], axis=1)\n",
    "#    df = pd.concat([df, df_rerepetition], axis=1)\n",
    "#    df.columns = ['emg1', 'emg2', 'emg3', 'emg4', 'emg5', 'emg6', 'emg7', 'emg8', 'emg9', 'emg10', 'emg11', 'emg12', 'emg13', 'emg14', 'emg15', 'emg16', 'Restimulus', 'rerepetition']\n",
    "    df.columns = ['emg1', 'emg2', 'emg3', 'emg4', 'emg5', 'emg6', 'emg7', 'emg8', 'emg9', 'emg10', 'emg11', 'emg12', 'emg13', 'emg14', 'emg15', 'emg16', 'Restimulus']\n",
    "    ninapro_df = pd.concat([ninapro_df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1d8918c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "설정 확인: 200Hz -> 1000Hz 변환 (비율: 5.0배)\n",
      "데이터 변환 진행 중 (Splitting -> Upsampling -> Normalization)...\n",
      "------------------------------\n",
      "변환 완료!\n",
      "총 샘플(동작) 개수: 2041\n",
      "최대 시퀀스 길이 (Max Time Steps): 18880\n",
      "입력 데이터 Shape (X): (2041, 18880, 16)\n",
      "라벨 데이터 Shape (y): (2041,)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#upsampling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_emg_data(df):\n",
    "   \n",
    "    # 설정 값\n",
    "    ORIGINAL_FS = 200   # 기존 주파수\n",
    "    TARGET_FS = 1000    # 목표 주파수\n",
    "    RESAMPLE_RATIO = TARGET_FS / ORIGINAL_FS # 5.0\n",
    "\n",
    "    # 1. 데이터와 라벨 분리\n",
    "    # 마지막 컬럼이 Restimulus라고 가정\n",
    "    feature_cols = df.columns[:-1]  # EMG 채널 16개\n",
    "    label_col = df.columns[-1]      # Restimulus\n",
    "    \n",
    "    print(f\"설정 확인: {ORIGINAL_FS}Hz -> {TARGET_FS}Hz 변환 (비율: {RESAMPLE_RATIO}배)\")\n",
    "    \n",
    "    # 2. 동작(Class)별 구간 자르기\n",
    "    # 단순히 라벨값(0,1,2..)으로 묶는게 아니라, 시간 순서상 '연속된 동작' 단위로 잘라야 함\n",
    "    # 라벨이 변하는 지점마다 새로운 그룹 ID 부여\n",
    "    df['segment_id'] = (df[label_col] != df[label_col].shift()).cumsum()\n",
    "    \n",
    "    processed_data_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    # 그룹별 처리\n",
    "    print(\"데이터 변환 진행 중 (Splitting -> Upsampling -> Normalization)...\")\n",
    "    \n",
    "    for _, group in df.groupby('segment_id'):\n",
    "        # 현재 구간의 라벨\n",
    "        current_label = group[label_col].iloc[0]\n",
    "        \n",
    "        # (선택사항) 0번 클래스(Rest/휴식)가 학습에 불필요하다면 아래 주석 해제하여 건너뛰기\n",
    "        # if current_label == 0: continue\n",
    "        \n",
    "        # 해당 구간의 EMG 데이터 추출 (Numpy 변환)\n",
    "        raw_signal = group[feature_cols].values\n",
    "        \n",
    "        # 3. 1000Hz Upsampling\n",
    "        # 변환 후 샘플 수 계산: 현재 길이 * 5\n",
    "        new_length = int(len(raw_signal) * RESAMPLE_RATIO)\n",
    "        \n",
    "        if new_length > 0:\n",
    "            # Fourier method를 이용한 리샘플링 (신호 손실 최소화)\n",
    "            upsampled_signal = signal.resample(raw_signal, new_length)\n",
    "            \n",
    "            # 4. Z-score 정규화 (Standardization)\n",
    "            # 각 동작 구간(Segment)마다 독립적으로 정규화 수행\n",
    "            scaler = StandardScaler()\n",
    "            normalized_signal = scaler.fit_transform(upsampled_signal)\n",
    "            \n",
    "            processed_data_list.append(normalized_signal)\n",
    "            labels_list.append(current_label)\n",
    "            \n",
    "    # 5. 모델 입력을 위한 Padding 및 Numpy Stacking\n",
    "    # 모델에 넣으려면 모든 데이터의 Time Step(길이)이 같아야 함.\n",
    "    # 가장 긴 데이터를 기준으로 나머지는 0으로 채움 (Zero-padding)\n",
    "    \n",
    "    max_seq_len = max(len(d) for d in processed_data_list)\n",
    "    num_samples = len(processed_data_list)\n",
    "    num_channels = len(feature_cols) # 16\n",
    "    \n",
    "    # 결과 담을 빈 배열 생성 (Batch, Time, Channel)\n",
    "    X_padded = np.zeros((num_samples, max_seq_len, num_channels))\n",
    "    \n",
    "    # 데이터 채워넣기\n",
    "    for i, data in enumerate(processed_data_list):\n",
    "        length = len(data)\n",
    "        X_padded[i, :length, :] = data  # 앞쪽부터 데이터 채움 (Post-padding 0)\n",
    "        \n",
    "    y_labels = np.array(labels_list)\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(\"변환 완료!\")\n",
    "    print(f\"총 샘플(동작) 개수: {num_samples}\")\n",
    "    print(f\"최대 시퀀스 길이 (Max Time Steps): {max_seq_len}\")\n",
    "    print(f\"입력 데이터 Shape (X): {X_padded.shape}\") # (Batch, Time, 16)\n",
    "    print(f\"라벨 데이터 Shape (y): {y_labels.shape}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    return X_padded, y_labels\n",
    "\n",
    "\n",
    "X_up, y_up = preprocess_emg_data(ninapro_df)\n",
    "\n",
    "# 3. 모델 입력 확인\n",
    "# 1D CNN + Attention 모델에 입력으로 'X'를, 타겟으로 'y'를 사용하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86cd14ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nature_df1110 종료\n",
      "nature_df1111 종료\n",
      "nature_df1112 종료\n",
      "nature_df1113 종료\n",
      "nature_df1114 종료\n",
      "nature_df1115 종료\n",
      "nature_df1116 종료\n",
      "nature_df1117 종료\n",
      "nature_df1118 종료\n",
      "nature_df1119 종료\n",
      "nature_df11110 종료\n",
      "nature_df11111 종료\n",
      "nature_df11112 종료\n",
      "nature_df11113 종료\n",
      "nature_df11114 종료\n",
      "nature_df11115 종료\n",
      "nature_df11116 종료\n",
      "nature_df11117 종료\n",
      "nature_df11118 종료\n",
      "nature_df11119 종료\n",
      "nature_df11120 종료\n",
      "nature_df11121 종료\n",
      "nature_df11122 종료\n",
      "nature_df11123 종료\n",
      "nature_df11124 종료\n",
      "nature_df11125 종료\n",
      "nature_df11126 종료\n",
      "nature_df11127 종료\n",
      "nature_df11128 종료\n",
      "nature_df11129 종료\n",
      "nature_df11130 종료\n",
      "nature_df11131 종료\n",
      "nature_df11132 종료\n",
      "nature_df11133 종료\n",
      "nature_df11134 종료\n",
      "nature_df11135 종료\n",
      "nature_df11136 종료\n",
      "nature_df11137 종료\n",
      "nature_df11138 종료\n",
      "nature_df11139 종료\n",
      "nature_df11140 종료\n",
      "nature_df11141 종료\n",
      "nature_df11142 종료\n",
      "nature_df11143 종료\n",
      "nature_df11144 종료\n",
      "nature_df11145 종료\n",
      "nature_df11146 종료\n",
      "nature_df11147 종료\n",
      "nature_df11148 종료\n",
      "nature_df11149 종료\n",
      "nature_df11150 종료\n",
      "nature_df11151 종료\n",
      "nature_df11152 종료\n",
      "nature_df11153 종료\n",
      "nature_df11154 종료\n",
      "nature_df11155 종료\n",
      "nature_df11156 종료\n",
      "nature_df11157 종료\n",
      "nature_df11158 종료\n",
      "nature_df11159 종료\n",
      "nature_df11160 종료\n",
      "nature_df11161 종료\n",
      "nature_df11162 종료\n",
      "nature_df11163 종료\n",
      "nature_df11164 종료\n",
      "nature_df11165 종료\n",
      "nature_df11166 종료\n",
      "nature_df11167 종료\n",
      "nature_df11168 종료\n",
      "nature_df11169 종료\n",
      "nature_df11170 종료\n",
      "nature_df11171 종료\n",
      "nature_df11172 종료\n",
      "nature_df11173 종료\n",
      "nature_df11174 종료\n",
      "nature_df11175 종료\n",
      "nature_df11176 종료\n",
      "nature_df11177 종료\n",
      "nature_df11178 종료\n",
      "nature_df11179 종료\n",
      "nature_df11180 종료\n",
      "nature_df11181 종료\n",
      "nature_df11182 종료\n",
      "nature_df11183 종료\n",
      "nature_df11184 종료\n",
      "nature_df11185 종료\n",
      "nature_df11186 종료\n",
      "nature_df11187 종료\n",
      "nature_df11188 종료\n",
      "nature_df11189 종료\n",
      "nature_df11190 종료\n",
      "nature_df11191 종료\n",
      "nature_df11192 종료\n",
      "nature_df11193 종료\n",
      "nature_df11194 종료\n",
      "nature_df11195 종료\n",
      "nature_df11196 종료\n",
      "nature_df11197 종료\n",
      "nature_df11198 종료\n",
      "nature_df11199 종료\n",
      "nature_df111100 종료\n",
      "nature_df111101 종료\n",
      "nature_df111102 종료\n",
      "nature_df111103 종료\n",
      "nature_df111104 종료\n",
      "nature_df111105 종료\n",
      "nature_df111106 종료\n",
      "nature_df111107 종료\n",
      "nature_df111108 종료\n",
      "nature_df111109 종료\n",
      "nature_df111110 종료\n",
      "nature_df111111 종료\n",
      "nature_df111112 종료\n",
      "nature_df111113 종료\n",
      "nature_df111114 종료\n",
      "nature_df111115 종료\n",
      "nature_df111116 종료\n",
      "nature_df111117 종료\n",
      "nature_df111118 종료\n",
      "nature_df111119 종료\n",
      "nature_df111120 종료\n",
      "nature_df111121 종료\n",
      "nature_df111122 종료\n",
      "nature_df111123 종료\n",
      "nature_df111124 종료\n",
      "nature_df111125 종료\n",
      "nature_df111126 종료\n",
      "nature_df111127 종료\n",
      "nature_df111128 종료\n",
      "nature_df111129 종료\n",
      "nature_df111130 종료\n",
      "nature_df111131 종료\n",
      "nature_df111132 종료\n",
      "nature_df111133 종료\n",
      "nature_df111134 종료\n",
      "nature_df111135 종료\n",
      "nature_df111136 종료\n",
      "nature_df111137 종료\n",
      "nature_df111138 종료\n",
      "nature_df111139 종료\n",
      "nature_df111140 종료\n",
      "nature_df111141 종료\n",
      "nature_df111142 종료\n",
      "nature_df111143 종료\n",
      "nature_df111144 종료\n",
      "nature_df111145 종료\n",
      "nature_df111146 종료\n",
      "nature_df111147 종료\n",
      "nature_df111148 종료\n",
      "nature_df111149 종료\n",
      "nature_df1120 종료\n",
      "nature_df1121 종료\n",
      "nature_df1122 종료\n",
      "nature_df1123 종료\n",
      "nature_df1124 종료\n",
      "nature_df1125 종료\n",
      "nature_df1126 종료\n",
      "nature_df1127 종료\n",
      "nature_df1128 종료\n",
      "nature_df1129 종료\n",
      "nature_df11210 종료\n",
      "nature_df11211 종료\n",
      "nature_df11212 종료\n",
      "nature_df11213 종료\n",
      "nature_df11214 종료\n",
      "nature_df11215 종료\n",
      "nature_df11216 종료\n",
      "nature_df11217 종료\n",
      "nature_df11218 종료\n",
      "nature_df11219 종료\n",
      "nature_df11220 종료\n",
      "nature_df11221 종료\n",
      "nature_df11222 종료\n",
      "nature_df11223 종료\n",
      "nature_df11224 종료\n",
      "nature_df11225 종료\n",
      "nature_df11226 종료\n",
      "nature_df11227 종료\n",
      "nature_df11228 종료\n",
      "nature_df11229 종료\n",
      "nature_df11230 종료\n",
      "nature_df11231 종료\n",
      "nature_df11232 종료\n",
      "nature_df11233 종료\n",
      "nature_df11234 종료\n",
      "nature_df11235 종료\n",
      "nature_df11236 종료\n",
      "nature_df11237 종료\n",
      "nature_df11238 종료\n",
      "nature_df11239 종료\n",
      "nature_df11240 종료\n",
      "nature_df11241 종료\n",
      "nature_df11242 종료\n",
      "nature_df11243 종료\n",
      "nature_df11244 종료\n",
      "nature_df11245 종료\n",
      "nature_df11246 종료\n",
      "nature_df11247 종료\n",
      "nature_df11248 종료\n",
      "nature_df11249 종료\n",
      "nature_df11250 종료\n",
      "nature_df11251 종료\n",
      "nature_df11252 종료\n",
      "nature_df11253 종료\n",
      "nature_df11254 종료\n",
      "nature_df11255 종료\n",
      "nature_df11256 종료\n",
      "nature_df11257 종료\n",
      "nature_df11258 종료\n",
      "nature_df11259 종료\n",
      "nature_df11260 종료\n",
      "nature_df11261 종료\n",
      "nature_df11262 종료\n",
      "nature_df11263 종료\n",
      "nature_df11264 종료\n",
      "nature_df11265 종료\n",
      "nature_df11266 종료\n",
      "nature_df11267 종료\n",
      "nature_df11268 종료\n",
      "nature_df11269 종료\n",
      "nature_df11270 종료\n",
      "nature_df11271 종료\n",
      "nature_df11272 종료\n",
      "nature_df11273 종료\n",
      "nature_df11274 종료\n",
      "nature_df11275 종료\n",
      "nature_df11276 종료\n",
      "nature_df11277 종료\n",
      "nature_df11278 종료\n",
      "nature_df11279 종료\n",
      "nature_df11280 종료\n",
      "nature_df11281 종료\n",
      "nature_df11282 종료\n",
      "nature_df11283 종료\n",
      "nature_df11284 종료\n",
      "nature_df11285 종료\n",
      "nature_df11286 종료\n",
      "nature_df11287 종료\n",
      "nature_df11288 종료\n",
      "nature_df11289 종료\n",
      "nature_df11290 종료\n",
      "nature_df11291 종료\n",
      "nature_df11292 종료\n",
      "nature_df11293 종료\n",
      "nature_df11294 종료\n",
      "nature_df11295 종료\n",
      "nature_df11296 종료\n",
      "nature_df11297 종료\n",
      "nature_df11298 종료\n",
      "nature_df11299 종료\n",
      "nature_df112100 종료\n",
      "nature_df112101 종료\n",
      "nature_df112102 종료\n",
      "nature_df112103 종료\n",
      "nature_df112104 종료\n",
      "nature_df112105 종료\n",
      "nature_df112106 종료\n",
      "nature_df112107 종료\n",
      "nature_df112108 종료\n",
      "nature_df112109 종료\n",
      "nature_df112110 종료\n",
      "nature_df112111 종료\n",
      "nature_df112112 종료\n",
      "nature_df112113 종료\n",
      "nature_df112114 종료\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m data_parame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrasp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data_parame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrasp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(grasp_mapping)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m150\u001b[39m):\n\u001b[1;32m---> 20\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnature_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43ml\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     21\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[0;32m     22\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRestimulus\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\dlrmflf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\h5py\\_hl\\dataset.py:1144\u001b[0m, in \u001b[0;36mDataset.__array__\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   1140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1141\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset.__array__ received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcopy\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut memory allocation cannot be avoided on read\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1143\u001b[0m     )\n\u001b[1;32m-> 1144\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;66;03m# Special case for (0,)*-shape datasets\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#nature 데이터셋 불러오기\n",
    "grasp_mapping = {\n",
    "    1: 6,\n",
    "    2: 18,\n",
    "    3: 7,\n",
    "    4: 5,\n",
    "    5: 19,\n",
    "    6: 0\n",
    "}\n",
    "\n",
    "nature_df = pd.DataFrame()\n",
    "for i in range(1, 9):\n",
    "    for j in range(1,3):\n",
    "        for k in range(1,3):\n",
    "            filename = fr'nature_data\\data\\participant_{i}\\participant{i}_day{j}_block{k}\\emg_data.hdf5'\n",
    "            data_parame = pd.read_csv(fr'nature_data\\data\\participant_{i}\\participant{i}_day{j}_block{k}\\trials.csv') \n",
    "            nature_data = h5py.File(filename, 'r')\n",
    "            data_parame['grasp'] = data_parame['grasp'].replace(grasp_mapping)\n",
    "            for l in range(0, 150):\n",
    "                df = pd.DataFrame(np.array(nature_data[f\"{l}\"]))\n",
    "                df=df.transpose()\n",
    "                df['Restimulus'] = ''\n",
    "                df['Restimulus'] = data_parame['grasp'].iloc[l]\n",
    "                nature_df = pd.concat([nature_df, df], axis=0)\n",
    "                print(f\"nature_df{i}{j}{k}{l} 종료\")\n",
    "nature_df.columns = ['emg1', 'emg2', 'emg3', 'emg4', 'emg5', 'emg6', 'emg7', 'emg8', 'emg9', 'emg10', 'emg11', 'emg12', 'emg13', 'emg14', 'emg15', 'emg16', 'Restimulus']\n",
    "#nature_df.to_csv('nature_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06b70f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emg1</th>\n",
       "      <th>emg2</th>\n",
       "      <th>emg3</th>\n",
       "      <th>emg4</th>\n",
       "      <th>emg5</th>\n",
       "      <th>emg6</th>\n",
       "      <th>emg7</th>\n",
       "      <th>emg8</th>\n",
       "      <th>emg9</th>\n",
       "      <th>emg10</th>\n",
       "      <th>emg11</th>\n",
       "      <th>emg12</th>\n",
       "      <th>emg13</th>\n",
       "      <th>emg14</th>\n",
       "      <th>emg15</th>\n",
       "      <th>emg16</th>\n",
       "      <th>Restimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47973915</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47973916</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47973917</th>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47973918</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47973919</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47973920 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              emg1      emg2      emg3      emg4      emg5      emg6  \\\n",
       "0         0.000038  0.000025  0.000008  0.000008 -0.000016 -0.000002   \n",
       "1         0.000020  0.000026  0.000010  0.000008 -0.000017 -0.000002   \n",
       "2         0.000009  0.000026  0.000011  0.000008 -0.000024 -0.000004   \n",
       "3         0.000012  0.000023  0.000009  0.000007 -0.000041 -0.000009   \n",
       "4         0.000021  0.000020  0.000006  0.000005 -0.000044 -0.000015   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "47973915  0.000030  0.000042  0.000010  0.000012 -0.000030 -0.000011   \n",
       "47973916  0.000030  0.000060  0.000011  0.000013 -0.000030 -0.000009   \n",
       "47973917  0.000024  0.000074  0.000012  0.000014 -0.000029 -0.000006   \n",
       "47973918  0.000015  0.000077  0.000013  0.000016 -0.000027 -0.000005   \n",
       "47973919  0.000002  0.000076  0.000014  0.000018 -0.000026 -0.000006   \n",
       "\n",
       "              emg7      emg8      emg9     emg10     emg11     emg12  \\\n",
       "0        -0.000005 -0.000010  0.000017  0.000061 -0.000008 -0.000008   \n",
       "1        -0.000008 -0.000007  0.000014  0.000043 -0.000011 -0.000009   \n",
       "2        -0.000013 -0.000005  0.000014  0.000025 -0.000015 -0.000011   \n",
       "3        -0.000016 -0.000006  0.000012  0.000010 -0.000017 -0.000012   \n",
       "4        -0.000018 -0.000008  0.000007  0.000003 -0.000016 -0.000011   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "47973915 -0.000027 -0.000002 -0.000092 -0.000030 -0.000013 -0.000016   \n",
       "47973916 -0.000026 -0.000013 -0.000086 -0.000012 -0.000010 -0.000015   \n",
       "47973917 -0.000025 -0.000014 -0.000072  0.000002 -0.000010 -0.000016   \n",
       "47973918 -0.000021 -0.000014 -0.000051  0.000002 -0.000012 -0.000017   \n",
       "47973919 -0.000016 -0.000017 -0.000028 -0.000007 -0.000014 -0.000018   \n",
       "\n",
       "             emg13     emg14     emg15     emg16  Restimulus  \n",
       "0        -0.000022 -0.000008 -0.000016 -0.000013           7  \n",
       "1        -0.000022 -0.000012 -0.000016 -0.000018           7  \n",
       "2        -0.000018 -0.000013 -0.000013 -0.000016           7  \n",
       "3        -0.000014 -0.000013 -0.000008 -0.000013           7  \n",
       "4        -0.000012 -0.000014 -0.000006 -0.000012           7  \n",
       "...            ...       ...       ...       ...         ...  \n",
       "47973915 -0.000010 -0.000008 -0.000013 -0.000011          18  \n",
       "47973916 -0.000009 -0.000008 -0.000014 -0.000011          18  \n",
       "47973917 -0.000011 -0.000008 -0.000014 -0.000013          18  \n",
       "47973918 -0.000013 -0.000006 -0.000013 -0.000014          18  \n",
       "47973919 -0.000014 -0.000006 -0.000012 -0.000015          18  \n",
       "\n",
       "[47973920 rows x 17 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nature df csv불러오는 코드\n",
    "nature_df = pd.read_csv('nature_df.csv')\n",
    "nature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04ecbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 처리 시작: 47973920 rows\n",
      "Sampling Rate 변환: 2000Hz -> 1000Hz (Downsampling)\n",
      "총 815개의 동작 세그먼트가 발견되었습니다. 변환 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Segments: 100%|██████████| 815/815 [00:37<00:00, 21.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패딩(Padding) 및 최종 배열 생성 중...\n",
      "----------------------------------------\n",
      "처리가 완료되었습니다.\n",
      "입력 데이터 Shape (X): (815, 125080, 16)\n",
      "라벨 데이터 Shape (y): (815,)\n",
      "메모리 사용량(예상): 6.08 GB\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#down sampling code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm  # 진행 상황 표시용 (설치 필요: pip install tqdm)\n",
    "\n",
    "def process_large_emg_data(df):\n",
    "    \"\"\"\n",
    "    4800만 행 규모의 2000Hz EMG 데이터를 처리하여 1D CNN 모델 입력용 Numpy 배열로 변환\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): 2000Hz Raw 데이터 (Rows x 17 cols)\n",
    "                           (1~16 col: EMG 센서, 17 col: Restimulus 클래스)\n",
    "    \n",
    "    Returns:\n",
    "        X_padded (np.array): (Batch_Size, Max_Time_Steps, 16)\n",
    "        y_labels (np.array): (Batch_Size,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 1. 설정 및 준비\n",
    "    # ---------------------------------------------------------\n",
    "    ORIGINAL_FS = 2000  # 원본 주파수\n",
    "    TARGET_FS = 1000    # 목표 주파수\n",
    "    RATIO = TARGET_FS / ORIGINAL_FS  # 0.5 (절반으로 축소)\n",
    "\n",
    "    feature_cols = df.columns[:-1]  # 앞의 16개 컬럼 (EMG)\n",
    "    label_col = df.columns[-1]      # 마지막 컬럼 (Restimulus)\n",
    "    \n",
    "    print(f\"데이터 처리 시작: {len(df)} rows\")\n",
    "    print(f\"Sampling Rate 변환: {ORIGINAL_FS}Hz -> {TARGET_FS}Hz (Downsampling)\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. 세그먼트 분할 (Segmentation)\n",
    "    # Restimulus 값이 바뀌는 지점을 기준으로 그룹 ID 생성\n",
    "    # ---------------------------------------------------------\n",
    "    # 대용량 데이터이므로 메모리 효율을 위해 필요한 컬럼만 사용하여 연산\n",
    "    df['segment_id'] = (df[label_col] != df[label_col].shift()).cumsum()\n",
    "    \n",
    "    processed_segments = []\n",
    "    labels = []\n",
    "    \n",
    "    # 그룹핑 (메모리 절약을 위해 groupby 객체를 바로 순회)\n",
    "    grouped = df.groupby('segment_id')\n",
    "    \n",
    "    print(f\"총 {len(grouped)}개의 동작 세그먼트가 발견되었습니다. 변환 중...\")\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 3. 반복 처리 (Downsampling -> Normalization)\n",
    "    # ---------------------------------------------------------\n",
    "    for _, group in tqdm(grouped, desc=\"Processing Segments\"):\n",
    "        # 현재 클래스 라벨\n",
    "        current_label = group[label_col].iloc[0]\n",
    "        \n",
    "        # (옵션) 0번(휴식) 클래스를 제외하려면 아래 주석 해제\n",
    "        # if current_label == 0: continue\n",
    "\n",
    "        # 센서 데이터 추출 (Numpy 변환)\n",
    "        raw_signals = group[feature_cols].to_numpy() # .values 보다 to_numpy() 권장\n",
    "        \n",
    "        # 데이터 길이가 너무 짧으면(예: 노이즈) 스킵하는 로직 추가 가능\n",
    "        if len(raw_signals) < 2:\n",
    "            continue\n",
    "\n",
    "        # [Downsampling] 2000Hz -> 1000Hz\n",
    "        # 목표 샘플 수 계산\n",
    "        new_num_samples = int(len(raw_signals) * RATIO)\n",
    "        \n",
    "        if new_num_samples > 0:\n",
    "            # Fourier method 리샘플링 (데이터가 많으므로 signal.decimate(x, 2)도 고려 가능)\n",
    "            # 여기서는 길이를 정확히 제어하기 위해 resample 사용\n",
    "            resampled_signals = signal.resample(raw_signals, new_num_samples)\n",
    "            \n",
    "            # [Z-score Normalization]\n",
    "            # 각 세그먼트별로 독립적으로 정규화\n",
    "            scaler = StandardScaler()\n",
    "            normalized_signals = scaler.fit_transform(resampled_signals)\n",
    "            \n",
    "            # 리스트에 저장 (float32로 변환하여 메모리 절약 권장)\n",
    "            processed_segments.append(normalized_signals.astype(np.float32))\n",
    "            labels.append(current_label)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4. 패딩 (Padding) 및 최종 변환\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"패딩(Padding) 및 최종 배열 생성 중...\")\n",
    "    \n",
    "    # 가장 긴 시퀀스 길이 찾기\n",
    "    max_len = max(len(seg) for seg in processed_segments)\n",
    "    num_samples = len(processed_segments)\n",
    "    num_channels = len(feature_cols)\n",
    "    \n",
    "    # 결과 배열 생성 (메모리 효율을 위해 float32 사용)\n",
    "    # Shape: (Batch_Size, Max_Time_Steps, Channels)\n",
    "    X_final = np.zeros((num_samples, max_len, num_channels), dtype=np.float32)\n",
    "    y_final = np.array(labels, dtype=np.int32)\n",
    "    \n",
    "    # 패딩 적용 (Post-padding: 뒤쪽을 0으로 채움)\n",
    "    for i, seg in enumerate(processed_segments):\n",
    "        length = len(seg)\n",
    "        X_final[i, :length, :] = seg\n",
    "        \n",
    "    print(\"-\" * 40)\n",
    "    print(\"처리가 완료되었습니다.\")\n",
    "    print(f\"입력 데이터 Shape (X): {X_final.shape}\")\n",
    "    print(f\"라벨 데이터 Shape (y): {y_final.shape}\")\n",
    "    print(f\"메모리 사용량(예상): {X_final.nbytes / 1024**3:.2f} GB\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    return X_final, y_final\n",
    "\n",
    "# 3. 함수 실행\n",
    "X_down, y_down = process_large_emg_data(nature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf83d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 3. 데이터 병합 (Merging) ===\n",
      "병합 시작...\n",
      "데이터셋 1 Shape: (2041, 18880, 16)\n",
      "데이터셋 2 Shape: (815, 125080, 16)\n",
      "통합 후 목표 Shape: (2856, 125080, 16)\n",
      "병합 완료.\n",
      "----------------------------------------\n",
      "최종 모델 입력 데이터 Shape (X): (2856, 125080, 16)\n",
      "최종 모델 라벨 데이터 Shape (y): (2856,)\n",
      "데이터 타입: float32\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#combin data code\n",
    "import numpy as np\n",
    "import gc  # 가비지 컬렉션(메모리 관리용)\n",
    "\n",
    "def merge_datasets(X1, y1, X2, y2):\n",
    "    \"\"\"\n",
    "    서로 다른 길이를 가진 두 개의 3차원 Numpy 배열을 병합합니다.\n",
    "    가장 긴 시퀀스 길이를 기준으로 Zero-padding을 수행합니다.\n",
    "    \"\"\"\n",
    "    print(f\"병합 시작...\")\n",
    "    print(f\"데이터셋 1 Shape: {X1.shape}\")\n",
    "    print(f\"데이터셋 2 Shape: {X2.shape}\")\n",
    "\n",
    "    # 1. 통합 차원 계산\n",
    "    # 샘플 수 합계\n",
    "    total_samples = X1.shape[0] + X2.shape[0]\n",
    "    # 두 데이터 중 더 긴 시퀀스 길이 선택 (여기서는 125080 예상)\n",
    "    max_seq_len = max(X1.shape[1], X2.shape[1])\n",
    "    num_channels = X1.shape[2] # 16\n",
    "\n",
    "    print(f\"통합 후 목표 Shape: ({total_samples}, {max_seq_len}, {num_channels})\")\n",
    "    \n",
    "    # 2. 메모리 할당 (Float32로 메모리 절약)\n",
    "    # 한 번에 큰 메모리를 할당하고 데이터를 밀어넣는 방식이 가장 효율적입니다.\n",
    "    X_final = np.zeros((total_samples, max_seq_len, num_channels), dtype=np.float32)\n",
    "    y_final = np.zeros((total_samples,), dtype=y1.dtype) # 라벨용\n",
    "\n",
    "    # 3. 데이터 채워넣기 (Padding 자동 적용)\n",
    "    # X1 (업샘플링 데이터) 넣기\n",
    "    # X1의 길이만큼만 앞부분에 채워지고, 나머지는 0으로 남음 (Post-padding 효과)\n",
    "    len1 = X1.shape[1]\n",
    "    X_final[:X1.shape[0], :len1, :] = X1\n",
    "    y_final[:y1.shape[0]] = y1\n",
    "\n",
    "    # X2 (다운샘플링 데이터) 넣기\n",
    "    # X1이 끝난 지점부터 시작\n",
    "    len2 = X2.shape[1]\n",
    "    X_final[X1.shape[0]:, :len2, :] = X2\n",
    "    y_final[y1.shape[0]:] = y2\n",
    "\n",
    "    print(\"병합 완료.\")\n",
    "    return X_final, y_final\n",
    "\n",
    "# ========================================================\n",
    "# [실행 코드] 변수명 분리 및 병합\n",
    "# ========================================================\n",
    "\n",
    "print(\"\\n=== 3. 데이터 병합 (Merging) ===\")\n",
    "# 병합 함수 실행\n",
    "X_combined, y_combined = merge_datasets(X_up, y_up, X_down, y_down)\n",
    "\n",
    "# 메모리 확보를 위해 이전 변수 삭제 (선택사항)\n",
    "del X_up, y_up, X_down, y_down\n",
    "gc.collect()\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"최종 모델 입력 데이터 Shape (X): {X_combined.shape}\")\n",
    "print(f\"최종 모델 라벨 데이터 Shape (y): {y_combined.shape}\")\n",
    "print(f\"데이터 타입: {X_combined.dtype}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 이 X_combined, y_combined를 1D CNN Encoder Attention 모델에 넣으시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e407eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 저장을 시작합니다. 대상 폴더: ./processed_data\n",
      "1. X_combined.npy 저장 중... (시간이 소요될 수 있습니다)\n",
      "2. y_combined.npy 저장 중...\n",
      "일반 저장 완료.\n",
      "3. 압축 저장 시작 (권장): ./processed_data\\emg_data_compressed.npz ...\n",
      "모든 저장이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 저장할 폴더 생성 (현재 경로에 data 폴더 생성)\n",
    "save_dir = './processed_data'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "print(f\"데이터 저장을 시작합니다. 대상 폴더: {save_dir}\")\n",
    "\n",
    "# ========================================================\n",
    "# Method 1: 일반 .npy 파일로 저장 (속도 빠름, 용량 큼)\n",
    "# ========================================================\n",
    "# X_combined.npy와 y_combined.npy로 각각 저장됩니다.\n",
    "# 예상 용량: 약 23GB\n",
    "\n",
    "print(\"1. X_combined.npy 저장 중... (시간이 소요될 수 있습니다)\")\n",
    "np.save(os.path.join(save_dir, 'X_combined.npy'), X_combined)\n",
    "\n",
    "print(\"2. y_combined.npy 저장 중...\")\n",
    "np.save(os.path.join(save_dir, 'y_combined.npy'), y_combined)\n",
    "\n",
    "print(\"일반 저장 완료.\")\n",
    "\n",
    "# ========================================================\n",
    "# Method 2: 압축된 .npz 파일로 저장 (권장)\n",
    "# ========================================================\n",
    "# 패딩으로 채워진 0이 많으므로 용량이 1/10 이하로 줄어들 가능성이 큽니다.\n",
    "# 나중에 로드할 때는 np.load()['X'] 형태로 불러옵니다.\n",
    "\n",
    "save_path_compressed = os.path.join(save_dir, 'emg_data_compressed.npz')\n",
    "print(f\"3. 압축 저장 시작 (권장): {save_path_compressed} ...\")\n",
    "\n",
    "np.savez_compressed(save_path_compressed, X=X_combined, y=y_combined)\n",
    "\n",
    "print(\"모든 저장이 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935e140c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 개수 자동 탐색 중...\n",
      "-> 감지된 최대 라벨: 19\n",
      "-> 설정된 NUM_CLASSES: 20\n",
      "Input Shape per Sample: torch.Size([16, 125080])\n",
      "모델 학습 시작 (Device: cpu)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/90 [00:00<?, ?it/s]C:\\Users\\dlrmflf\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# 1. Memory-Efficient Dataset (대용량 처리용)\n",
    "# ==========================================\n",
    "class LargeEMGDataset(Dataset):\n",
    "    def __init__(self, x_path, y_path):\n",
    "        \"\"\"\n",
    "        mmap_mode='r'을 사용하여 데이터를 메모리에 다 올리지 않고\n",
    "        디스크에서 필요한 부분만 읽어옵니다.\n",
    "        \"\"\"\n",
    "        # 실제 데이터 로드는 아니고 포인터만 생성\n",
    "        self.X = np.load(x_path, mmap_mode='r')\n",
    "        self.y = np.load(y_path, mmap_mode='r')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. 디스크에서 해당 인덱스 데이터만 읽음\n",
    "        # 데이터 원본 Shape: (Batch, Time, Channel) -> (125080, 16) 등\n",
    "        data = self.X[idx].astype(np.float32)\n",
    "        label = self.y[idx].astype(np.longlong)\n",
    "        \n",
    "        # 2. PyTorch 포맷으로 변환 (Channel-First)\n",
    "        # 입력: (Time, Channel) -> 출력: (Channel, Time)\n",
    "        # 주의: 1D CNN은 (Batch, Channel, Time) 형태를 원하지만,\n",
    "        # 우리 모델 내부에서 다시 (Batch*Channel, 1, Time)으로 바꿀 것이므로\n",
    "        # 여기서는 (Channel, Time)으로 넘겨줍니다.\n",
    "        data_tensor = torch.from_numpy(data).transpose(0, 1) \n",
    "        \n",
    "        return data_tensor, torch.tensor(label)\n",
    "\n",
    "# ==========================================\n",
    "# 2. Model Architecture (이론 기반 구현)\n",
    "# ==========================================\n",
    "class ChannelWiseEncoderAttention(nn.Module):\n",
    "    def __init__(self, time_steps, num_classes, embedding_dim=64):\n",
    "        super(ChannelWiseEncoderAttention, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # [2.2] Channel-wise Shared Encoder (f_theta)\n",
    "        # 모든 채널이 이 파라미터를 공유합니다.\n",
    "        # Input: (Batch * Channels, 1, Time)\n",
    "        self.shared_encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=64, stride=2, padding=32),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            nn.Conv1d(16, 32, kernel_size=32, stride=2, padding=16),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=16, stride=2, padding=8),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1), # Time 차원을 1로 압축\n",
    "            nn.Flatten()             # (B*C, 64)\n",
    "        )\n",
    "        \n",
    "        # Encoder 출력 차원을 embedding_dim으로 맞춤\n",
    "        self.projection = nn.Linear(64, embedding_dim)\n",
    "\n",
    "        # [2.3] Attention Mechanism (Aggregation)\n",
    "        # 채널별 중요도(Scalar)를 계산하기 위한 레이어\n",
    "        self.attn_layer = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 1) # 스칼라 점수 출력\n",
    "        )\n",
    "        \n",
    "        # [Classifier]\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x: (Batch, Channels, Time)\n",
    "        B, C, T = x.shape\n",
    "        \n",
    "        # -------------------------------------------------------\n",
    "        # Step 1: Reshape for Shared Encoder (Channel-wise Processing)\n",
    "        # 각 채널을 독립적인 샘플로 취급: (Batch * Channels, 1, Time)\n",
    "        # -------------------------------------------------------\n",
    "        x_reshaped = x.view(B * C, 1, T)\n",
    "        \n",
    "        # -------------------------------------------------------\n",
    "        # Step 2: Encode (f_theta)\n",
    "        # h: (Batch * Channels, Embedding_Dim)\n",
    "        # -------------------------------------------------------\n",
    "        features = self.shared_encoder(x_reshaped)\n",
    "        features = self.projection(features)\n",
    "        \n",
    "        # 다시 배치 단위로 복구: (Batch, Channels, Embedding_Dim)\n",
    "        features = features.view(B, C, self.embedding_dim)\n",
    "        \n",
    "        # -------------------------------------------------------\n",
    "        # Step 3: Attention Pooling (Aggregation)\n",
    "        # 각 채널의 feature에 대해 중요도(alpha) 계산\n",
    "        # -------------------------------------------------------\n",
    "        # scores: (Batch, Channels, 1)\n",
    "        scores = self.attn_layer(features) \n",
    "        \n",
    "        # weights: (Batch, Channels, 1) - 채널 차원(dim=1)에 대해 Softmax\n",
    "        attn_weights = torch.softmax(scores, dim=1)\n",
    "        \n",
    "        # Weighted Sum: (Batch, Embedding_Dim)\n",
    "        # z = Sum(alpha_c * h_c)\n",
    "        z = (features * attn_weights).sum(dim=1)\n",
    "        \n",
    "        # -------------------------------------------------------\n",
    "        # Step 4: Classifier\n",
    "        # -------------------------------------------------------\n",
    "        logits = self.classifier(z)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# ==========================================\n",
    "# 3. Execution Script\n",
    "# ==========================================\n",
    "def run_training():\n",
    "    # 파일 경로 설정\n",
    "    X_PATH = './processed_data/X_combined.npy'\n",
    "    Y_PATH = './processed_data/y_combined.npy'\n",
    "    \n",
    "    # 1. 데이터셋 준비 (mmap 사용)\n",
    "    # y 데이터를 먼저 살짝 읽어서 클래스 개수를 파악합니다.\n",
    "    print(\"클래스 개수 자동 탐색 중...\")\n",
    "    y_temp = np.load(Y_PATH, mmap_mode='r')\n",
    "    \n",
    "    # 라벨 중 가장 큰 값 + 1 이 총 클래스 개수가 됩니다.\n",
    "    # (예: 라벨이 0~13 이라면 max는 13이고, 클래스 개수는 14개)\n",
    "    MAX_LABEL = int(np.max(y_temp))\n",
    "    NUM_CLASSES = MAX_LABEL + 1\n",
    "    \n",
    "    print(f\"-> 감지된 최대 라벨: {MAX_LABEL}\")\n",
    "    print(f\"-> 설정된 NUM_CLASSES: {NUM_CLASSES}\")\n",
    "\n",
    "    dataset = LargeEMGDataset(X_PATH, Y_PATH)\n",
    "    \n",
    "    # Shape 확인\n",
    "    temp_x, temp_y = dataset[0] \n",
    "    CHANNELS, TIME_STEPS = temp_x.shape\n",
    "    \n",
    "    print(f\"Input Shape per Sample: {temp_x.shape}\") \n",
    "    \n",
    "    # 2. DataLoader 설정 (Windows 에러 방지: num_workers=0)\n",
    "    BATCH_SIZE = 32 \n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=0,      # Windows Error 해결을 위해 0으로 설정\n",
    "        pin_memory=True     # GPU 전송 속도 향상\n",
    "    )\n",
    "    \n",
    "    # 3. 모델 초기화 (자동 감지된 NUM_CLASSES 주입)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = ChannelWiseEncoderAttention(\n",
    "        time_steps=TIME_STEPS, \n",
    "        num_classes=NUM_CLASSES,  # <--- 수정된 부분\n",
    "        embedding_dim=128\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(f\"모델 학습 시작 (Device: {device})\")\n",
    "    \n",
    "    # 4. 학습 루프\n",
    "    EPOCHS = 5\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        \n",
    "        for inputs, labels in loop:\n",
    "            inputs = inputs.to(device) # (B, C, T)\n",
    "            labels = labels.to(device) # (B,)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            loop.set_postfix(loss=running_loss/len(dataloader), acc=100.*correct/total)\n",
    "\n",
    "    print(\"학습 완료.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64fc7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
